{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Embeddings con Transformers\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duración estimada:** 2-2.5 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [De Word2Vec a Transformers](#intro)\n",
    "2. [BERT: El primer Transformer contextual](#bert)\n",
    "3. [Sentence Transformers](#sentence)\n",
    "   - 3.1 Modelos Open Source\n",
    "   - 3.2 Búsqueda semántica\n",
    "4. [Visualización con UMAP](#umap)\n",
    "5. [Embeddings de proveedores comerciales](#comercial)\n",
    "6. [Ejercicios prácticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, serás capaz de:\n",
    "- Comprender la diferencia entre embeddings estáticos y contextuales\n",
    "- Utilizar BERT y Sentence Transformers para generar embeddings\n",
    "- Implementar búsqueda semántica con similitud coseno\n",
    "- Visualizar embeddings en 3D con UMAP\n",
    "- Conocer las opciones comerciales disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. De Word2Vec a Transformers\n",
    "\n",
    "### El problema de los embeddings estáticos\n",
    "\n",
    "En el notebook anterior vimos Word2Vec y GloVe. Estos modelos tienen una limitación fundamental: **generan un único vector por palabra**, sin importar el contexto.\n",
    "\n",
    "**Ejemplo del problema:**\n",
    "- \"Voy al **banco** a sacar dinero\" (institución financiera)\n",
    "- \"Me senté en el **banco** del parque\" (asiento)\n",
    "- \"Vimos un **banco** de peces\" (grupo de animales)\n",
    "\n",
    "En Word2Vec, las tres oraciones usarían el **mismo vector** para \"banco\", perdiendo el significado contextual.\n",
    "\n",
    "### Evolución histórica\n",
    "\n",
    "| Modelo | Año | Tipo | Características |\n",
    "|--------|-----|------|----------------|\n",
    "| Word2Vec | 2013 | Estático | Un vector por palabra |\n",
    "| ELMo | 2018 | Contextual (BiLSTM) | Primer modelo contextual |\n",
    "| BERT | 2018 | Contextual (Transformer) | Bidireccional, atención |\n",
    "| Sentence-BERT | 2019 | Sentence-level | Embeddings de oraciones |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de dependencias\n",
    "\n",
    "Todas las librerías usadas son **gratuitas y open source**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado: 'C:\\\\Users\\\\frane\\\\anaconda3\\\\Lib\\\\site-packages\\\\~l_dtypes\\\\_ml_dtypes_ext.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (all free and open source)\n",
    "!pip install -q transformers sentence-transformers torch\n",
    "!pip install -q umap-learn plotly scikit-learn\n",
    "!pip install -q tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bert\"></a>\n",
    "## 2. BERT: El primer Transformer contextual\n",
    "\n",
    "### ¿Qué es BERT?\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) fue desarrollado por Google en 2018. Sus características principales son:\n",
    "\n",
    "- **Bidireccional**: Lee toda la oración simultáneamente (no izquierda-a-derecha)\n",
    "- **Contextual**: Genera diferentes embeddings según el contexto\n",
    "- **Pre-entrenado**: Entrenado en grandes corpus, ajustable para tareas específicas\n",
    "\n",
    "### Arquitectura de BERT\n",
    "\n",
    "| Modelo | Capas | Heads | Parámetros | Dimensión |\n",
    "|--------|-------|-------|------------|----------|\n",
    "| BERT-Base | 12 | 12 | 110M | 768 |\n",
    "| BERT-Large | 24 | 16 | 340M | 1024 |\n",
    "\n",
    "### Tareas de pre-entrenamiento\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**: Predecir palabras enmascaradas\n",
    "   - Entrada: \"El [MASK] ladra fuerte\"\n",
    "   - Salida: \"perro\"\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**: ¿Es B la siguiente oración de A?\n",
    "   - A: \"El cliente hizo el pedido.\"\n",
    "   - B: \"El sistema registró el pago.\" → [IsNext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy>=1.17,<2.0 is required for a normal functioning of this module, but found numpy==2.2.6.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load pre-trained BERT model (English)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCargando BERT base (inglés)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[1;34m(requirement)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 111\u001b[0m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[1;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is unusual. Consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy>=1.17,<2.0 is required for a normal functioning of this module, but found numpy==2.2.6.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model (English)\n",
    "print(\"Cargando BERT base (inglés)...\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Modelo cargado: bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get BERT embeddings\n",
    "text = \"The bank approved my loan application\"\n",
    "\n",
    "# Tokenize and get embeddings\n",
    "tokens = tokenizer_bert(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_bert(**tokens)\n",
    "\n",
    "# Get embeddings from last hidden state\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(f\"Texto: '{text}'\")\n",
    "print(f\"Tokens: {tokenizer_bert.convert_ids_to_tokens(tokens['input_ids'][0])}\")\n",
    "print(f\"Shape de embeddings: {embeddings.shape}\")\n",
    "print(f\"  - Batch size: {embeddings.shape[0]}\")\n",
    "print(f\"  - Número de tokens: {embeddings.shape[1]}\")\n",
    "print(f\"  - Dimensión del embedding: {embeddings.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT en español\n",
    "\n",
    "Existen modelos BERT entrenados específicamente en español. Uno de los más populares es **BETO**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load Spanish BERT model (BETO)\n",
    "print(\"Cargando BETO (BERT español)...\")\n",
    "modelo_id = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer_es = AutoTokenizer.from_pretrained(modelo_id)\n",
    "model_es = AutoModel.from_pretrained(modelo_id)\n",
    "model_es.eval()\n",
    "\n",
    "print(f\"Modelo cargado: {modelo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with Spanish text\n",
    "texto_es = \"El banco central ajustó las tasas de interés\"\n",
    "\n",
    "tokens_es = tokenizer_es(texto_es, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs_es = model_es(**tokens_es)\n",
    "\n",
    "embeddings_es = outputs_es.last_hidden_state\n",
    "\n",
    "print(f\"Texto: '{texto_es}'\")\n",
    "print(f\"Tokens: {tokenizer_es.convert_ids_to_tokens(tokens_es['input_ids'][0])}\")\n",
    "print(f\"Shape de embeddings: {embeddings_es.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de modelos\n",
    "\n",
    "| Modelo | Idiomas | Embeddings Contextuales | Bidireccional | Subpalabras |\n",
    "|--------|---------|------------------------|---------------|-------------|\n",
    "| Word2Vec | Mono | ❌ | ❌ | ❌ |\n",
    "| GloVe | Mono | ❌ | ❌ | ❌ |\n",
    "| ELMo | Mono | ✅ | Parcial | ❌ |\n",
    "| BERT | Multi | ✅ | ✅ | ✅ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"sentence\"></a>\n",
    "## 3. Sentence Transformers\n",
    "\n",
    "### El problema de BERT para oraciones\n",
    "\n",
    "BERT genera un embedding **por cada token**. Para obtener un embedding de toda la oración, necesitamos agregar estos vectores (promedio, CLS token, etc.), lo cual no es óptimo.\n",
    "\n",
    "### ¿Qué es Sentence Transformers?\n",
    "\n",
    "**Sentence Transformers** es una librería construida sobre HuggingFace que:\n",
    "- Genera un **único vector** por oración/párrafo\n",
    "- Optimizado para **similitud semántica**\n",
    "- Perfecto para búsqueda semántica, clustering, clasificación\n",
    "\n",
    "### Modelos populares\n",
    "\n",
    "| Modelo | Idiomas | Dimensiones | Uso recomendado |\n",
    "|--------|---------|-------------|----------------|\n",
    "| all-MiniLM-L6-v2 | Multi | 384 | Rápido, buena precisión |\n",
    "| all-mpnet-base-v2 | Multi | 768 | Mejor precisión |\n",
    "| paraphrase-multilingual-MiniLM-L12-v2 | Multi | 384 | Multilingüe |\n",
    "| hiiamsid/sentence_similarity_spanish_es | ES | 768 | Español específico |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Sentence Transformer model (free, open source)\n",
    "print(\"Cargando modelo Sentence Transformer...\")\n",
    "model_st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Modelo cargado: all-MiniLM-L6-v2\")\n",
    "print(f\"Dimensión del embedding: 384\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence embedding\n",
    "sentence = \"Artificial intelligence is transforming the world.\"\n",
    "embedding = model_st.encode(sentence)\n",
    "\n",
    "print(f\"Oración: '{sentence}'\")\n",
    "print(f\"Tipo: {type(embedding)}\")\n",
    "print(f\"Shape: {embedding.shape}\")\n",
    "print(f\"Primeros 10 valores: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Búsqueda Semántica\n",
    "\n",
    "Una de las aplicaciones más importantes de Sentence Transformers es la **búsqueda semántica**: encontrar documentos similares a una consulta basándose en el significado, no solo en palabras clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "# Query and candidate sentences\n",
    "query = \"¿Cómo puedo abrir una cuenta bancaria?\"\n",
    "\n",
    "candidates = [\n",
    "    \"Para abrir una cuenta bancaria, visita una sucursal con tu DNI.\",\n",
    "    \"Las cuentas bancarias se abren en línea fácilmente.\",\n",
    "    \"El banco central bajó los tipos de interés.\",\n",
    "    \"Me gusta el helado de vainilla.\",\n",
    "    \"Este es un buen banco para sentarse.\",\n",
    "    \"Ayer vi nadando en el mar un banco de peces.\",\n",
    "    \"Los requisitos incluyen identificación y comprobante de domicilio.\",\n",
    "    \"El clima está muy agradable hoy.\"\n",
    "]\n",
    "\n",
    "print(f\"Consulta: '{query}'\")\n",
    "print(f\"\\nCandidatos: {len(candidates)} oraciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode query and candidates\n",
    "query_embedding = model_st.encode(query, convert_to_tensor=True)\n",
    "candidate_embeddings = model_st.encode(candidates, convert_to_tensor=True)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, candidate_embeddings)[0]\n",
    "\n",
    "# Sort by similarity\n",
    "results = sorted(zip(candidates, similarities.tolist()), \n",
    "                key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Resultados ordenados por similitud semántica:\")\n",
    "print(\"=\" * 60)\n",
    "for sentence, score in results:\n",
    "    emoji = \"✅\" if score > 0.4 else \"❌\"\n",
    "    print(f\"{emoji} {score:.4f} | {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones de la búsqueda semántica\n",
    "\n",
    "Nota cómo el modelo:\n",
    "- **Entiende el contexto**: Relaciona \"abrir cuenta\" con \"requisitos\" e \"identificación\"\n",
    "- **Distingue homónimos**: Diferencia \"banco\" (financiero) de \"banco\" (asiento) y \"banco\" (peces)\n",
    "- **Ignora irrelevantes**: Baja puntuación para oraciones sin relación semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for semantic search\n",
    "def semantic_search(query, documents, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search on documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - query: search query string\n",
    "    - documents: list of document strings\n",
    "    - model: SentenceTransformer model\n",
    "    - top_k: number of results to return\n",
    "    \n",
    "    Returns:\n",
    "    - list of (document, score) tuples\n",
    "    \"\"\"\n",
    "    query_emb = model.encode(query, convert_to_tensor=True)\n",
    "    doc_embs = model.encode(documents, convert_to_tensor=True)\n",
    "    \n",
    "    similarities = util.cos_sim(query_emb, doc_embs)[0]\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    \n",
    "    results = [(documents[i], similarities[i].item()) for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "new_query = \"What documents do I need for banking?\"\n",
    "results = semantic_search(new_query, candidates, model_st, top_k=3)\n",
    "\n",
    "print(f\"Query: '{new_query}'\\n\")\n",
    "for doc, score in results:\n",
    "    print(f\"  {score:.4f} | {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"umap\"></a>\n",
    "## 4. Visualización con UMAP\n",
    "\n",
    "**UMAP** (Uniform Manifold Approximation and Projection) es un algoritmo de reducción de dimensionalidad que preserva mejor la estructura local que PCA, ideal para visualizar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import plotly.express as px\n",
    "\n",
    "# Sample sentences for visualization\n",
    "sentences = [\n",
    "    # Animals\n",
    "    \"The dog barks in the garden\",\n",
    "    \"The cat sleeps on the couch\",\n",
    "    \"The bird sings in the morning\",\n",
    "    \"The horse runs in the field\",\n",
    "    # Technology\n",
    "    \"Artificial intelligence is advancing rapidly\",\n",
    "    \"Machine learning requires lots of data\",\n",
    "    \"Neural networks learn patterns automatically\",\n",
    "    \"Deep learning powers modern AI applications\",\n",
    "    # Finance\n",
    "    \"The stock market crashed yesterday\",\n",
    "    \"Banks offer different interest rates\",\n",
    "    \"Investment requires careful planning\",\n",
    "    \"The economy is recovering slowly\",\n",
    "    # Food\n",
    "    \"Pizza is my favorite food\",\n",
    "    \"I love eating sushi for dinner\",\n",
    "    \"The restaurant serves excellent pasta\",\n",
    "    \"Cooking at home is healthier\"\n",
    "]\n",
    "\n",
    "categories = (\n",
    "    [\"Animals\"] * 4 + \n",
    "    [\"Technology\"] * 4 + \n",
    "    [\"Finance\"] * 4 + \n",
    "    [\"Food\"] * 4\n",
    ")\n",
    "\n",
    "print(f\"Total de oraciones: {len(sentences)}\")\n",
    "print(f\"Categorías: {set(categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings_viz = model_st.encode(sentences)\n",
    "print(f\"Shape de embeddings: {embeddings_viz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP for 3D visualization\n",
    "reducer = umap.UMAP(n_components=3, random_state=42, n_neighbors=5)\n",
    "embeddings_3d = reducer.fit_transform(embeddings_viz)\n",
    "\n",
    "print(f\"Shape después de UMAP: {embeddings_3d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization with Plotly\n",
    "fig = px.scatter_3d(\n",
    "    x=embeddings_3d[:, 0],\n",
    "    y=embeddings_3d[:, 1],\n",
    "    z=embeddings_3d[:, 2],\n",
    "    text=sentences,\n",
    "    color=categories,\n",
    "    title=\"Embeddings de oraciones (Sentence-BERT + UMAP 3D)\",\n",
    "    labels={'color': 'Categoría'}\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP 1',\n",
    "        yaxis_title='UMAP 2',\n",
    "        zaxis_title='UMAP 3'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones de la visualización\n",
    "\n",
    "En el gráfico 3D podemos observar:\n",
    "- Las oraciones de la misma categoría tienden a agruparse\n",
    "- UMAP preserva la estructura semántica mejor que PCA\n",
    "- Los clusters están bien separados\n",
    "\n",
    "Esto demuestra que los Sentence Transformers capturan efectivamente el significado semántico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"comercial\"></a>\n",
    "## 5. Embeddings de proveedores comerciales (Opcional)\n",
    "\n",
    "Además de los modelos open source, existen embeddings comerciales de alta calidad:\n",
    "\n",
    "### OpenAI Embeddings\n",
    "| Modelo | Dimensiones | Precio (aprox) |\n",
    "|--------|-------------|----------------|\n",
    "| text-embedding-3-small | 1536 | $0.00002/1K tokens |\n",
    "| text-embedding-3-large | 3072 | $0.00013/1K tokens |\n",
    "\n",
    "### Google Gemini Embeddings\n",
    "| Modelo | Dimensiones | Precio |\n",
    "|--------|-------------|--------|\n",
    "| text-embedding-004 | 768 | Gratis (límites) |\n",
    "| gemini-embedding-exp | 768-3072 | Variable |\n",
    "\n",
    "**Nota:** Para este curso, usamos modelos open source para evitar costos. Los modelos comerciales pueden ofrecer mejor rendimiento en algunos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for OpenAI embeddings (requires API key)\n",
    "# Uncomment and add your API key to use\n",
    "\n",
    "# from openai import OpenAI\n",
    "# \n",
    "# OPENAI_API_KEY = \"your-api-key-here\"\n",
    "# client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "# \n",
    "# response = client.embeddings.create(\n",
    "#     input=\"Sample text for embedding\",\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "# \n",
    "# embedding = response.data[0].embedding\n",
    "# print(f\"OpenAI embedding dimension: {len(embedding)}\")\n",
    "\n",
    "print(\"El código para embeddings comerciales está comentado.\")\n",
    "print(\"Para usarlo, descomenta y añade tu API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 6. Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Búsqueda semántica en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a semantic search system for FAQs\n",
    "\n",
    "faqs = [\n",
    "    \"¿Cuál es el horario de atención al cliente?\",\n",
    "    \"¿Cómo puedo cambiar mi contraseña?\",\n",
    "    \"¿Cuáles son los métodos de pago aceptados?\",\n",
    "    \"¿Cómo realizo una devolución de producto?\",\n",
    "    \"¿Tienen envío internacional?\",\n",
    "    \"¿Cuánto tiempo tarda el envío?\",\n",
    "    \"¿Puedo cancelar mi pedido?\",\n",
    "    \"¿Cómo contacto con soporte técnico?\"\n",
    "]\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"Quiero devolver algo que compré\",\n",
    "    \"¿Aceptan tarjeta de crédito?\",\n",
    "    \"Olvidé mi clave de acceso\"\n",
    "]\n",
    "\n",
    "print(\"Sistema de FAQ con búsqueda semántica\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    results = semantic_search(query, faqs, model_st, top_k=2)\n",
    "    print(f\"\\nPregunta: '{query}'\")\n",
    "    print(\"Respuestas más relevantes:\")\n",
    "    for faq, score in results:\n",
    "        print(f\"  {score:.4f} | {faq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Comparar diferentes modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compare embedding dimensions and speed\n",
    "import time\n",
    "\n",
    "models_to_compare = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "]\n",
    "\n",
    "test_texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Natural language processing enables computers to understand text.\"\n",
    "]\n",
    "\n",
    "print(\"Comparación de modelos:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    start = time.time()\n",
    "    embeddings = model.encode(test_texts)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"  Dimensión: {embeddings.shape[1]}\")\n",
    "    print(f\"  Tiempo: {elapsed:.4f}s para {len(test_texts)} textos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Clustering semántico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Semantic clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "mixed_sentences = [\n",
    "    # Tech (cluster 0)\n",
    "    \"Python is a popular programming language\",\n",
    "    \"JavaScript runs in web browsers\",\n",
    "    \"Machine learning requires data\",\n",
    "    # Sports (cluster 1)  \n",
    "    \"Football is played with 11 players\",\n",
    "    \"Basketball requires a hoop and ball\",\n",
    "    \"Tennis is played on a court\",\n",
    "    # Food (cluster 2)\n",
    "    \"Pizza originated in Italy\",\n",
    "    \"Sushi is a Japanese dish\",\n",
    "    \"Tacos are popular in Mexico\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embs = model_st.encode(mixed_sentences)\n",
    "\n",
    "# Apply K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(embs)\n",
    "\n",
    "print(\"Clustering semántico (sin etiquetas previas):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cluster_id in range(3):\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for sent, c in zip(mixed_sentences, clusters):\n",
    "        if c == cluster_id:\n",
    "            print(f\"  - {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **Embeddings contextuales**: BERT genera diferentes vectores según el contexto\n",
    "2. **Sentence Transformers**: Embeddings optimizados para oraciones completas\n",
    "3. **Búsqueda semántica**: Encontrar documentos por significado, no palabras\n",
    "4. **Visualización UMAP**: Reducción de dimensionalidad preservando estructura\n",
    "\n",
    "### Cuándo usar cada modelo\n",
    "\n",
    "| Caso de uso | Modelo recomendado |\n",
    "|-------------|-------------------|\n",
    "| Búsqueda semántica rápida | all-MiniLM-L6-v2 |\n",
    "| Multilingüe | paraphrase-multilingual-MiniLM-L12-v2 |\n",
    "| Máxima precisión | all-mpnet-base-v2 |\n",
    "| Español específico | BETO o hiiamsid/sentence_similarity_spanish_es |\n",
    "\n",
    "En el siguiente notebook veremos **Ingeniería de Prompts**, fundamental para trabajar con LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [BERT Paper (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)\n",
    "- [Sentence-BERT Paper (Reimers & Gurevych, 2019)](https://arxiv.org/abs/1908.10084)\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [HuggingFace Transformers](https://huggingface.co/docs/transformers/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
