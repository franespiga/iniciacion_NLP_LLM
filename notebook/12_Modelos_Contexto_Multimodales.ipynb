{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Modelos de Gran Contexto y Multimodales\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duración estimada:** 2 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Modelos de Gran Contexto (LCM)](#lcm)\n",
    "2. [LCM vs RAG](#comparacion)\n",
    "3. [Modelos Multimodales](#multimodal)\n",
    "4. [CLIP: Imágenes + Texto](#clip)\n",
    "5. [Aplicaciones prácticas](#aplicaciones)\n",
    "6. [El futuro de los LLMs](#futuro)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, serás capaz de:\n",
    "- Entender las ventajas de modelos con gran ventana de contexto\n",
    "- Decidir cuándo usar LCM vs RAG\n",
    "- Comprender modelos multimodales como CLIP\n",
    "- Implementar búsqueda de imágenes por texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lcm\"></a>\n",
    "## 1. Modelos de Gran Contexto (LCM)\n",
    "\n",
    "### ¿Qué es la ventana de contexto?\n",
    "\n",
    "La **ventana de contexto** es el número máximo de tokens que un modelo puede procesar en una sola llamada.\n",
    "\n",
    "| Modelo | Ventana de contexto |\n",
    "|--------|--------------------|\n",
    "| GPT-3.5 | 4K - 16K tokens |\n",
    "| GPT-4 Turbo | 128K tokens |\n",
    "| Claude 3 | 200K tokens |\n",
    "| Gemini 1.5 Pro | 1M tokens |\n",
    "| Llama 3.1 | 128K tokens |\n",
    "\n",
    "### ¿Qué permite un contexto grande?\n",
    "\n",
    "- Procesar documentos largos completos\n",
    "- Analizar libros enteros\n",
    "- Mantener conversaciones muy largas\n",
    "- Razonar sobre múltiples documentos a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Token counter\n",
    "def count_tokens(text, model=\"gpt-4\"):\n",
    "    \"\"\"Count tokens in text for a given model.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Example texts\n",
    "short_text = \"Hola, ¿cómo estás?\"\n",
    "medium_text = \"La inteligencia artificial es una rama de la informática que busca crear sistemas capaces de realizar tareas que normalmente requieren inteligencia humana. Esto incluye el aprendizaje, el razonamiento, la percepción y el lenguaje natural.\" * 10\n",
    "long_text = medium_text * 50\n",
    "\n",
    "print(f\"Texto corto: {count_tokens(short_text)} tokens\")\n",
    "print(f\"Texto medio: {count_tokens(medium_text)} tokens\")\n",
    "print(f\"Texto largo: {count_tokens(long_text)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"comparacion\"></a>\n",
    "## 2. LCM vs RAG\n",
    "\n",
    "### Comparación\n",
    "\n",
    "| Aspecto | Gran Contexto (LCM) | RAG |\n",
    "|---------|---------------------|-----|\n",
    "| **Acceso a info** | Todo en el prompt | Recuperación selectiva |\n",
    "| **Persistencia** | No (se pierde) | Sí (vector store) |\n",
    "| **Costo** | Alto (muchos tokens) | Menor |\n",
    "| **Latencia** | Mayor | Menor |\n",
    "| **Precisión** | Puede diluirse | Enfocada |\n",
    "| **Actualización** | Requiere re-enviar | Fácil añadir docs |\n",
    "\n",
    "### ¿Cuándo usar cada uno?\n",
    "\n",
    "**Usa LCM cuando:**\n",
    "- El documento cabe en la ventana\n",
    "- Necesitas razonamiento global\n",
    "- La información está interconectada\n",
    "\n",
    "**Usa RAG cuando:**\n",
    "- Tienes muchos documentos\n",
    "- La información cambia frecuentemente\n",
    "- Necesitas citaciones precisas\n",
    "- El costo es importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Token cost estimation\n",
    "def estimate_cost(tokens, model=\"gpt-4-turbo\"):\n",
    "    \"\"\"Estimate API cost for different models.\"\"\"\n",
    "    costs = {\n",
    "        \"gpt-4-turbo\": 0.01 / 1000,  # $0.01 per 1K input tokens\n",
    "        \"gpt-3.5-turbo\": 0.0005 / 1000,\n",
    "        \"claude-3-opus\": 0.015 / 1000,\n",
    "    }\n",
    "    return tokens * costs.get(model, 0.01/1000)\n",
    "\n",
    "# Compare costs\n",
    "doc_tokens = 50000  # 50K tokens (medium document)\n",
    "\n",
    "print(f\"Documento de {doc_tokens:,} tokens\")\n",
    "print(f\"\\nCosto LCM (todo el documento):\")\n",
    "for model in [\"gpt-4-turbo\", \"gpt-3.5-turbo\", \"claude-3-opus\"]:\n",
    "    cost = estimate_cost(doc_tokens, model)\n",
    "    print(f\"  {model}: ${cost:.4f}\")\n",
    "\n",
    "print(f\"\\nCosto RAG (solo chunks relevantes, ~2000 tokens):\")\n",
    "for model in [\"gpt-4-turbo\", \"gpt-3.5-turbo\", \"claude-3-opus\"]:\n",
    "    cost = estimate_cost(2000, model)\n",
    "    print(f\"  {model}: ${cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"multimodal\"></a>\n",
    "## 3. Modelos Multimodales\n",
    "\n",
    "Los modelos **multimodales** pueden procesar múltiples tipos de datos:\n",
    "- Texto\n",
    "- Imágenes\n",
    "- Audio\n",
    "- Video\n",
    "\n",
    "### Ejemplos de modelos multimodales\n",
    "\n",
    "| Modelo | Modalidades | Uso |\n",
    "|--------|-------------|-----|\n",
    "| GPT-4V | Texto + Imágenes | Análisis visual, OCR |\n",
    "| CLIP | Texto + Imágenes | Búsqueda, clasificación |\n",
    "| Whisper | Audio → Texto | Transcripción |\n",
    "| DALL-E | Texto → Imágenes | Generación de imágenes |\n",
    "| Gemini | Texto + Imágenes + Audio + Video | General |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"clip\"></a>\n",
    "## 4. CLIP: Imágenes + Texto\n",
    "\n",
    "**CLIP** (Contrastive Language-Image Pre-training) aprende la relación entre imágenes y texto.\n",
    "\n",
    "### Arquitectura\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐\n",
    "│   Imagen    │     │   Texto     │\n",
    "└──────┬──────┘     └──────┬──────┘\n",
    "       ↓                   ↓\n",
    "┌──────┴──────┐     ┌──────┴──────┐\n",
    "│  Encoder    │     │  Encoder    │\n",
    "│  Visual     │     │  Texto      │\n",
    "└──────┬──────┘     └──────┬──────┘\n",
    "       ↓                   ↓\n",
    "   [Vector]           [Vector]\n",
    "       └───────┬───────┘\n",
    "               ↓\n",
    "        [Similitud]\n",
    "```\n",
    "\n",
    "### Aplicaciones\n",
    "\n",
    "- Búsqueda de imágenes por texto\n",
    "- Clasificación zero-shot\n",
    "- Generación de imágenes (guía para DALL-E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CLIP\n",
    "!pip install -q ftfy regex tqdm\n",
    "!pip install -q git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "print(f\"CLIP cargado en {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample images\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load image from URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "# Sample image URLs (using placeholder service)\n",
    "image_urls = {\n",
    "    \"dog\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/YellowLabradorLooking_new.jpg/1200px-YellowLabradorLooking_new.jpg\",\n",
    "    \"cat\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
    "}\n",
    "\n",
    "# Load images (may fail if URLs are blocked, that's OK)\n",
    "try:\n",
    "    images = {name: load_image_from_url(url) for name, url in image_urls.items()}\n",
    "    print(f\"Imágenes cargadas: {list(images.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudieron cargar imágenes: {e}\")\n",
    "    print(\"Continuando con ejemplo teórico...\")\n",
    "    images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Zero-shot classification with CLIP\n",
    "def classify_image(image, labels, model, preprocess, device):\n",
    "    \"\"\"Classify image using CLIP zero-shot.\"\"\"\n",
    "    # Preprocess image\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Tokenize labels\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {label}\" for label in labels]).to(device)\n",
    "    \n",
    "    # Get features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return {label: prob.item() for label, prob in zip(labels, similarity[0])}\n",
    "\n",
    "# Test classification\n",
    "if images:\n",
    "    labels = [\"dog\", \"cat\", \"bird\", \"car\"]\n",
    "    for name, img in images.items():\n",
    "        results = classify_image(img, labels, model, preprocess, device)\n",
    "        print(f\"\\nImagen: {name}\")\n",
    "        for label, prob in sorted(results.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {label}: {prob:.2%}\")\n",
    "else:\n",
    "    print(\"Ejemplo teórico: CLIP puede clasificar imágenes sin entrenamiento específico\")\n",
    "    print(\"Simplemente comparando embeddings de imagen y texto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de imágenes por texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Image search by text\n",
    "def search_images_by_text(query, image_features, image_names, model, device):\n",
    "    \"\"\"Search images using text query.\"\"\"\n",
    "    # Encode query\n",
    "    text_input = clip.tokenize([query]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_input)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
    "    \n",
    "    results = list(zip(image_names, similarities[0].tolist()))\n",
    "    return sorted(results, key=lambda x: -x[1])\n",
    "\n",
    "# Pre-compute image features\n",
    "if images:\n",
    "    image_names = list(images.keys())\n",
    "    image_tensors = torch.cat([preprocess(img).unsqueeze(0) for img in images.values()]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensors)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Search\n",
    "    queries = [\"a fluffy animal\", \"a pet that barks\", \"a feline\"]\n",
    "    \n",
    "    for query in queries:\n",
    "        results = search_images_by_text(query, image_features, image_names, model, device)\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        for name, score in results:\n",
    "            print(f\"  {name}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"aplicaciones\"></a>\n",
    "## 5. Aplicaciones prácticas\n",
    "\n",
    "### Casos de uso de modelos multimodales\n",
    "\n",
    "| Aplicación | Modelo | Descripción |\n",
    "|------------|--------|-------------|\n",
    "| OCR inteligente | GPT-4V | Extraer texto de imágenes con contexto |\n",
    "| Búsqueda visual | CLIP | Buscar productos por imagen |\n",
    "| Accesibilidad | GPT-4V | Describir imágenes para usuarios ciegos |\n",
    "| E-commerce | CLIP | Clasificación automática de productos |\n",
    "| Seguridad | CLIP | Moderación de contenido visual |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"futuro\"></a>\n",
    "## 6. El Futuro de los LLMs\n",
    "\n",
    "### Tendencias actuales\n",
    "\n",
    "1. **Ventanas de contexto más grandes**: De 4K a 1M+ tokens\n",
    "2. **Multimodalidad nativa**: Un modelo para todo\n",
    "3. **Agentes autónomos**: LLMs que ejecutan tareas complejas\n",
    "4. **Modelos más pequeños y eficientes**: Phi-3, Llama 3 8B\n",
    "5. **Fine-tuning accesible**: LoRA, QLoRA\n",
    "6. **Razonamiento mejorado**: Chain of Thought, Tree of Thought\n",
    "\n",
    "### Arquitecturas emergentes\n",
    "\n",
    "- **Mixture of Experts (MoE)**: Múltiples modelos especializados\n",
    "- **State Space Models**: Mamba, alternativa a Transformers\n",
    "- **Retrieval-Augmented Models**: RAG integrado en el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Curso\n",
    "\n",
    "### Lo que hemos aprendido\n",
    "\n",
    "| Notebook | Temas |\n",
    "|----------|-------|\n",
    "| 01 | Embeddings básicos: One-Hot, Word2Vec, GloVe |\n",
    "| 02 | Transformers: BERT, Sentence Transformers |\n",
    "| 03 | Ingeniería de Prompts |\n",
    "| 04 | Chatbots básicos |\n",
    "| 05 | Vector Stores y Retrieval |\n",
    "| 06 | Introducción a RAG |\n",
    "| 07 | Reranking y optimización |\n",
    "| 08 | Introducción a Agentes |\n",
    "| 09 | Agentes con LangChain |\n",
    "| 10 | LangGraph y flujos |\n",
    "| 11 | RAG Avanzado Agentic |\n",
    "| 12 | Modelos de contexto y multimodales |\n",
    "\n",
    "### Recursos para continuar aprendiendo\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Hugging Face Course](https://huggingface.co/learn/nlp-course)\n",
    "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### ¡Felicidades por completar el curso!\n",
    "\n",
    "Ahora tienes las bases para construir aplicaciones de IA con LLMs. El campo evoluciona rápidamente, así que sigue practicando y experimentando.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n",
    "- [Long Context Models](https://arxiv.org/abs/2307.03172)\n",
    "- [Multimodal Models Survey](https://arxiv.org/abs/2306.13549)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
