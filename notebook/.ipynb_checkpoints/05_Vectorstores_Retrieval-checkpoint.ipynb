{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Vector Stores y Retrieval\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duración estimada:** 2-2.5 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introducción a Vector Stores](#intro)\n",
    "2. [FAISS: Vector Store local](#faiss)\n",
    "3. [Document Loaders](#loaders)\n",
    "4. [Text Splitters](#splitters)\n",
    "5. [Similarity Search](#search)\n",
    "6. [Persistencia y carga](#persistencia)\n",
    "7. [Ejercicios prácticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, serás capaz de:\n",
    "- Entender qué son los vector stores y cómo funcionan\n",
    "- Crear y usar FAISS para búsqueda vectorial\n",
    "- Cargar documentos desde diferentes fuentes\n",
    "- Dividir documentos en chunks óptimos\n",
    "- Realizar búsquedas semánticas eficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Introducción a Vector Stores\n",
    "\n",
    "### ¿Qué es un Vector Store?\n",
    "\n",
    "Un **Vector Store** (base de datos vectorial) es un sistema diseñado para almacenar, indexar y buscar vectores de alta dimensionalidad de forma eficiente.\n",
    "\n",
    "### ¿Por qué son importantes?\n",
    "\n",
    "Los LLMs tienen conocimiento limitado a su fecha de entrenamiento y no conocen datos privados/corporativos. Los vector stores permiten:\n",
    "\n",
    "1. **Búsqueda semántica**: Encontrar documentos por significado, no keywords\n",
    "2. **Memoria a largo plazo**: Almacenar conocimiento externo\n",
    "3. **RAG**: Retrieval-Augmented Generation\n",
    "\n",
    "### Opciones populares\n",
    "\n",
    "| Vector Store | Tipo | Características |\n",
    "|--------------|------|----------------|\n",
    "| **FAISS** | Local | Gratuito, rápido, en memoria |\n",
    "| **Chroma** | Local | Gratuito, persistente, fácil de usar |\n",
    "| **Pinecone** | Cloud | Escalable, managed, pago |\n",
    "| **Weaviate** | Cloud/Local | Open source, GraphQL |\n",
    "| **Qdrant** | Cloud/Local | Open source, Rust |\n",
    "\n",
    "Para este curso usamos **FAISS** por ser gratuito y no requerir infraestructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (all free)\n",
    "!pip install -q langchain langchain-community langchain-huggingface\n",
    "!pip install -q faiss-cpu sentence-transformers\n",
    "!pip install -q beautifulsoup4 pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Librerías instaladas ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"faiss\"></a>\n",
    "## 2. FAISS: Vector Store local\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) es una librería desarrollada por Meta para búsqueda eficiente de vectores similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load free embedding model\n",
    "print(\"Cargando modelo de embeddings (gratuito)...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_vector = embeddings.embed_query(\"Hello world\")\n",
    "print(f\"Dimensión del embedding: {len(test_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python es un lenguaje de programación interpretado y de alto nivel.\",\n",
    "        metadata={\"source\": \"python.txt\", \"topic\": \"programming\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"JavaScript es el lenguaje de la web, ejecutado en navegadores.\",\n",
    "        metadata={\"source\": \"javascript.txt\", \"topic\": \"programming\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine Learning es un subcampo de la inteligencia artificial.\",\n",
    "        metadata={\"source\": \"ml.txt\", \"topic\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep Learning usa redes neuronales con múltiples capas.\",\n",
    "        metadata={\"source\": \"dl.txt\", \"topic\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"FAISS es una librería para búsqueda eficiente de vectores similares.\",\n",
    "        metadata={\"source\": \"faiss.txt\", \"topic\": \"tools\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Creados {len(documents)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "print(\"Creando vector store con FAISS...\")\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "print(f\"Vector store creado con {vector_store.index.ntotal} vectores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"loaders\"></a>\n",
    "## 3. Document Loaders\n",
    "\n",
    "LangChain proporciona loaders para cargar documentos desde múltiples fuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web page loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load content from a web page\n",
    "print(\"Cargando contenido web...\")\n",
    "loader = WebBaseLoader(\"https://es.wikipedia.org/wiki/Python\")\n",
    "web_docs = loader.load()\n",
    "\n",
    "print(f\"Documentos cargados: {len(web_docs)}\")\n",
    "print(f\"Caracteres: {len(web_docs[0].page_content)}\")\n",
    "print(f\"Metadata: {web_docs[0].metadata}\")\n",
    "print(f\"\\nPrimeros 500 caracteres:\\n{web_docs[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text file loader (create a sample file first)\n",
    "sample_text = \"\"\"Introducción a la Inteligencia Artificial\n",
    "\n",
    "La inteligencia artificial (IA) es una rama de la informática que busca crear \n",
    "sistemas capaces de realizar tareas que normalmente requieren inteligencia humana.\n",
    "\n",
    "Tipos de IA:\n",
    "1. IA Débil (Narrow AI): Diseñada para tareas específicas\n",
    "2. IA Fuerte (General AI): Capaz de cualquier tarea intelectual humana\n",
    "3. Superinteligencia: Hipotética IA que supera la inteligencia humana\n",
    "\n",
    "Aplicaciones comunes:\n",
    "- Reconocimiento de voz\n",
    "- Visión por computadora\n",
    "- Procesamiento de lenguaje natural\n",
    "- Sistemas de recomendación\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"sample_ai_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Load from file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "text_loader = TextLoader(\"sample_ai_doc.txt\", encoding=\"utf-8\")\n",
    "text_docs = text_loader.load()\n",
    "\n",
    "print(f\"Documento cargado: {len(text_docs)} archivo(s)\")\n",
    "print(f\"Contenido:\\n{text_docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"splitters\"></a>\n",
    "## 4. Text Splitters\n",
    "\n",
    "Los documentos largos deben dividirse en **chunks** (fragmentos) para:\n",
    "- Respetar límites de contexto del LLM\n",
    "- Mejorar precisión de búsqueda\n",
    "- Reducir costos de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,        # Maximum characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Priority of separators\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = text_splitter.split_documents(text_docs)\n",
    "\n",
    "print(f\"Documento original dividido en {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} ({len(chunk.page_content)} chars):\")\n",
    "    print(f\"  '{chunk.page_content[:80]}...'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different chunk sizes comparison\n",
    "sizes = [100, 200, 500]\n",
    "\n",
    "long_text = web_docs[0].page_content[:2000]  # First 2000 chars from Wikipedia\n",
    "\n",
    "print(\"Comparación de tamaños de chunk:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for size in sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size // 5  # 20% overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(long_text)\n",
    "    print(f\"chunk_size={size}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia de chunking\n",
    "\n",
    "| Chunk Size | Ventajas | Desventajas |\n",
    "|------------|----------|-------------|\n",
    "| Pequeño (100-300) | Más preciso, específico | Puede perder contexto |\n",
    "| Mediano (300-800) | Balance contexto/precisión | Uso general |\n",
    "| Grande (800-2000) | Más contexto | Menos preciso, más tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"search\"></a>\n",
    "## 5. Similarity Search\n",
    "\n",
    "La búsqueda por similitud es la operación fundamental de los vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store with more documents\n",
    "all_chunks = text_splitter.split_documents(text_docs)\n",
    "\n",
    "# Add more sample documents\n",
    "extra_docs = [\n",
    "    Document(page_content=\"Los transformers revolucionaron el NLP en 2017.\", \n",
    "             metadata={\"topic\": \"NLP\"}),\n",
    "    Document(page_content=\"GPT-4 es un modelo de lenguaje desarrollado por OpenAI.\",\n",
    "             metadata={\"topic\": \"LLM\"}),\n",
    "    Document(page_content=\"BERT es un modelo bidireccional pre-entrenado.\",\n",
    "             metadata={\"topic\": \"NLP\"}),\n",
    "    Document(page_content=\"Las redes convolucionales son excelentes para imágenes.\",\n",
    "             metadata={\"topic\": \"CV\"}),\n",
    "    Document(page_content=\"El aprendizaje por refuerzo entrena agentes mediante recompensas.\",\n",
    "             metadata={\"topic\": \"RL\"}),\n",
    "]\n",
    "\n",
    "all_docs = all_chunks + extra_docs\n",
    "\n",
    "# Create new vector store\n",
    "vs = FAISS.from_documents(all_docs, embeddings)\n",
    "print(f\"Vector store con {vs.index.ntotal} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic similarity search\n",
    "query = \"¿Qué es el procesamiento de lenguaje natural?\"\n",
    "\n",
    "results = vs.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nResultados:\")\n",
    "print(\"=\" * 50)\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "    print(f\"   Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search with scores\n",
    "results_with_scores = vs.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"\\nResultados con puntuación (menor = más similar):\")\n",
    "print(\"=\" * 50)\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"\\nScore: {score:.4f}\")\n",
    "    print(f\"  {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata filter\n",
    "# Note: FAISS doesn't support native filtering, but we can post-filter\n",
    "\n",
    "def search_with_filter(vector_store, query, k=10, filter_key=None, filter_value=None):\n",
    "    \"\"\"Search with optional metadata filtering.\"\"\"\n",
    "    # Get more results to filter\n",
    "    results = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    if filter_key and filter_value:\n",
    "        results = [doc for doc in results \n",
    "                   if doc.metadata.get(filter_key) == filter_value]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Search only NLP documents\n",
    "nlp_results = search_with_filter(vs, \"modelos de lenguaje\", filter_key=\"topic\", filter_value=\"NLP\")\n",
    "\n",
    "print(\"Búsqueda filtrada (topic='NLP'):\")\n",
    "for doc in nlp_results[:3]:\n",
    "    print(f\"  - {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever: Interfaz para búsqueda\n",
    "\n",
    "LangChain proporciona una interfaz `Retriever` para usar vector stores en chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vs.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Use retriever\n",
    "docs = retriever.invoke(\"inteligencia artificial\")\n",
    "\n",
    "print(f\"Retriever devolvió {len(docs)} documentos:\")\n",
    "for doc in docs:\n",
    "    print(f\"  - {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"persistencia\"></a>\n",
    "## 6. Persistencia y carga\n",
    "\n",
    "FAISS permite guardar y cargar el índice para no tener que recalcular embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vector store to disk\n",
    "vs.save_local(\"faiss_index\")\n",
    "print(\"Vector store guardado en 'faiss_index/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector store from disk\n",
    "loaded_vs = FAISS.load_local(\n",
    "    \"faiss_index\", \n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for pickle files\n",
    ")\n",
    "\n",
    "print(f\"Vector store cargado con {loaded_vs.index.ntotal} vectores\")\n",
    "\n",
    "# Test that it works\n",
    "test_results = loaded_vs.similarity_search(\"IA\", k=1)\n",
    "print(f\"Test búsqueda: {test_results[0].page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more documents to existing vector store\n",
    "new_docs = [\n",
    "    Document(page_content=\"LangChain facilita el desarrollo de aplicaciones con LLMs.\",\n",
    "             metadata={\"topic\": \"tools\"}),\n",
    "    Document(page_content=\"Hugging Face es la plataforma líder para modelos de ML.\",\n",
    "             metadata={\"topic\": \"tools\"}),\n",
    "]\n",
    "\n",
    "# Add to vector store\n",
    "loaded_vs.add_documents(new_docs)\n",
    "\n",
    "print(f\"Vectores después de añadir: {loaded_vs.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 7. Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Crear un buscador de FAQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: FAQ Search System\n",
    "\n",
    "faqs = [\n",
    "    {\"q\": \"¿Cómo puedo resetear mi contraseña?\", \n",
    "     \"a\": \"Ve a 'Olvidé mi contraseña' en la página de login.\"},\n",
    "    {\"q\": \"¿Cuál es el horario de atención?\",\n",
    "     \"a\": \"Lunes a Viernes de 9:00 a 18:00.\"},\n",
    "    {\"q\": \"¿Aceptan devoluciones?\",\n",
    "     \"a\": \"Sí, tienes 30 días para devolver productos sin usar.\"},\n",
    "    {\"q\": \"¿Tienen envío internacional?\",\n",
    "     \"a\": \"Sí, enviamos a más de 50 países.\"},\n",
    "    {\"q\": \"¿Cómo contacto con soporte?\",\n",
    "     \"a\": \"Email: soporte@ejemplo.com o chat en vivo.\"},\n",
    "]\n",
    "\n",
    "# Create documents with questions and answers\n",
    "faq_docs = [\n",
    "    Document(\n",
    "        page_content=faq[\"q\"],\n",
    "        metadata={\"answer\": faq[\"a\"]}\n",
    "    )\n",
    "    for faq in faqs\n",
    "]\n",
    "\n",
    "# Create vector store\n",
    "faq_vs = FAISS.from_documents(faq_docs, embeddings)\n",
    "\n",
    "# Search function\n",
    "def find_answer(query):\n",
    "    results = faq_vs.similarity_search(query, k=1)\n",
    "    if results:\n",
    "        return results[0].metadata[\"answer\"]\n",
    "    return \"No encontré una respuesta.\"\n",
    "\n",
    "# Test\n",
    "test_queries = [\n",
    "    \"olvidé mi clave\",\n",
    "    \"quiero devolver algo\",\n",
    "    \"¿envían a México?\"\n",
    "]\n",
    "\n",
    "print(\"Sistema FAQ:\")\n",
    "for q in test_queries:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {find_answer(q)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Comparar diferentes chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Experiment with chunk sizes\n",
    "# Load a longer document and test different chunk sizes\n",
    "\n",
    "# Create a longer sample document\n",
    "long_document = \"\"\"\n",
    "La inteligencia artificial ha transformado numerosas industrias en las últimas décadas.\n",
    "Desde el reconocimiento de voz hasta los vehículos autónomos, las aplicaciones son vastas.\n",
    "\n",
    "En el sector salud, la IA ayuda a diagnosticar enfermedades con mayor precisión.\n",
    "Los algoritmos pueden analizar imágenes médicas y detectar anomalías que los humanos podrían pasar por alto.\n",
    "\n",
    "En finanzas, los modelos predictivos ayudan a detectar fraudes y evaluar riesgos crediticios.\n",
    "Los chatbots atienden consultas de clientes las 24 horas del día.\n",
    "\n",
    "El comercio electrónico utiliza IA para personalizar recomendaciones de productos.\n",
    "Los sistemas analizan el historial de compras y navegación para sugerir artículos relevantes.\n",
    "\n",
    "Sin embargo, la IA también plantea desafíos éticos importantes.\n",
    "La privacidad de datos, el sesgo algorítmico y el desplazamiento laboral son temas críticos.\n",
    "\"\"\"\n",
    "\n",
    "# Test different chunk sizes and see search quality\n",
    "chunk_sizes = [100, 200, 400]\n",
    "query = \"aplicaciones de IA en salud\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=20)\n",
    "    chunks = splitter.create_documents([long_document])\n",
    "    vs_test = FAISS.from_documents(chunks, embeddings)\n",
    "    results = vs_test.similarity_search(query, k=1)\n",
    "    \n",
    "    print(f\"Chunk size={size}: {len(chunks)} chunks\")\n",
    "    print(f\"  Mejor resultado: '{results[0].page_content[:60]}...'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **Vector Stores**: Bases de datos para búsqueda semántica\n",
    "2. **FAISS**: Vector store local, rápido y gratuito\n",
    "3. **Document Loaders**: Cargar desde archivos, web, PDFs\n",
    "4. **Text Splitters**: Dividir documentos en chunks\n",
    "5. **Similarity Search**: Buscar por significado\n",
    "6. **Persistencia**: Guardar y cargar índices\n",
    "\n",
    "### Arquitectura típica de un sistema de retrieval\n",
    "\n",
    "```\n",
    "Documentos → [Loader] → [Splitter] → [Embeddings] → [Vector Store]\n",
    "                                                         ↓\n",
    "Query → [Embedding] → [Similarity Search] → Documentos relevantes\n",
    "```\n",
    "\n",
    "En el siguiente notebook veremos cómo combinar esto con LLMs para crear sistemas **RAG** (Retrieval-Augmented Generation).\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [FAISS Documentation](https://faiss.ai/)\n",
    "- [LangChain Vector Stores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "- [Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
