{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - RAG Avanzado: Agentic RAG\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**DuraciÃ³n estimada:** 2.5-3 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Ãndice\n",
    "\n",
    "1. [Â¿QuÃ© es Agentic RAG?](#intro)\n",
    "2. [Arquitectura completa](#arquitectura)\n",
    "3. [ImplementaciÃ³n paso a paso](#implementacion)\n",
    "4. [EvaluaciÃ³n con RAGAS](#evaluacion)\n",
    "5. [Mejores prÃ¡cticas de producciÃ³n](#produccion)\n",
    "6. [Ejercicio final](#ejercicio)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, serÃ¡s capaz de:\n",
    "- DiseÃ±ar sistemas RAG agentic completos\n",
    "- Implementar evaluaciÃ³n de calidad con RAGAS\n",
    "- Aplicar patrones de producciÃ³n\n",
    "- Optimizar el rendimiento del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Â¿QuÃ© es Agentic RAG?\n",
    "\n",
    "**Agentic RAG** combina las capacidades de:\n",
    "- **RAG**: Acceso a conocimiento externo\n",
    "- **Agentes**: Razonamiento y uso de herramientas\n",
    "- **Grafos**: Flujos de trabajo complejos\n",
    "\n",
    "### ComparaciÃ³n\n",
    "\n",
    "| RAG Simple | Agentic RAG |\n",
    "|------------|-------------|\n",
    "| Un paso de retrieval | MÃºltiples pasos adaptativos |\n",
    "| Sin razonamiento | Planifica y razona |\n",
    "| Query fija | Reformula queries |\n",
    "| Sin verificaciÃ³n | Auto-corrige respuestas |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-groq langgraph langchain-huggingface faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if 'GROQ_API_KEY' not in os.environ:\n",
    "    os.environ['GROQ_API_KEY'] = getpass(\"GROQ API Key: \")\n",
    "\n",
    "print(\"Configurado âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"arquitectura\"></a>\n",
    "## 2. Arquitectura Completa\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      AGENTIC RAG SYSTEM                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                â”‚\n",
    "â”‚  Query â†’ [Clasificar] â†’ [Reformular?] â†’ [Retrieval]           â”‚\n",
    "â”‚              â†“                              â†“                  â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”              [Verificar contexto]       â”‚\n",
    "â”‚     â”‚   FAQ?        â”‚                      â†“                  â”‚\n",
    "â”‚     â”‚   General?    â”‚              [Generar respuesta]        â”‚\n",
    "â”‚     â”‚   TÃ©cnica?    â”‚                      â†“                  â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              [Verificar calidad]        â”‚\n",
    "â”‚                                            â†“                  â”‚\n",
    "â”‚                                    [Corregir si necesario]    â”‚\n",
    "â”‚                                            â†“                  â”‚\n",
    "â”‚                                    [Respuesta final]          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"implementacion\"></a>\n",
    "## 3. ImplementaciÃ³n paso a paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup components\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict, List, Literal\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.3)\n",
    "\n",
    "# Create knowledge base\n",
    "knowledge_docs = [\n",
    "    Document(page_content=\"El IBI se paga anualmente y grava bienes inmuebles urbanos y rÃºsticos.\", \n",
    "             metadata={\"source\": \"ibi\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"El IVTM grava la titularidad de vehÃ­culos. Se paga mientras estÃ© matriculado.\",\n",
    "             metadata={\"source\": \"ivtm\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"Bonificaciones IBI: familias numerosas hasta 90%, energÃ­as renovables 50%.\",\n",
    "             metadata={\"source\": \"ibi\", \"type\": \"bonus\"}),\n",
    "    Document(page_content=\"Bonificaciones IVTM: vehÃ­culos elÃ©ctricos 75%, histÃ³ricos 100%.\",\n",
    "             metadata={\"source\": \"ivtm\", \"type\": \"bonus\"}),\n",
    "    Document(page_content=\"Plazos de pago: IBI segundo semestre, IVTM primer trimestre.\",\n",
    "             metadata={\"source\": \"general\", \"type\": \"deadlines\"}),\n",
    "    Document(page_content=\"Recursos: Puede interponer recurso en 30 dÃ­as desde la notificaciÃ³n.\",\n",
    "             metadata={\"source\": \"general\", \"type\": \"appeals\"}),\n",
    "]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(knowledge_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"Componentes inicializados âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state\n",
    "class AgenticRAGState(TypedDict):\n",
    "    question: str\n",
    "    question_type: str\n",
    "    reformulated_question: str\n",
    "    context: str\n",
    "    context_sufficient: bool\n",
    "    response: str\n",
    "    response_quality: str\n",
    "    final_response: str\n",
    "    iteration: int\n",
    "\n",
    "# Node functions\n",
    "def classify_question(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Classify the type of question.\"\"\"\n",
    "    prompt = f\"\"\"Clasifica esta pregunta en una de las categorÃ­as: DEFINITION, BONUS, DEADLINE, APPEAL, OTHER.\n",
    "    \n",
    "Pregunta: {state['question']}\n",
    "\n",
    "Responde solo con la categorÃ­a.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    q_type = response.content.strip().upper()\n",
    "    return {**state, \"question_type\": q_type}\n",
    "\n",
    "def reformulate_question(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Reformulate question for better retrieval.\"\"\"\n",
    "    prompt = f\"\"\"Reformula esta pregunta para mejorar la bÃºsqueda de informaciÃ³n.\n",
    "MantÃ©n la esencia pero hazla mÃ¡s especÃ­fica.\n",
    "\n",
    "Pregunta original: {state['question']}\n",
    "\n",
    "Pregunta reformulada:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"reformulated_question\": response.content.strip()}\n",
    "\n",
    "def retrieve_context(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Retrieve relevant documents.\"\"\"\n",
    "    query = state.get(\"reformulated_question\") or state[\"question\"]\n",
    "    docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([f\"- {d.page_content}\" for d in docs])\n",
    "    return {**state, \"context\": context}\n",
    "\n",
    "def check_context(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Check if context is sufficient.\"\"\"\n",
    "    prompt = f\"\"\"Â¿El contexto contiene informaciÃ³n relevante para responder la pregunta?\n",
    "    \n",
    "Pregunta: {state['question']}\n",
    "Contexto: {state['context']}\n",
    "\n",
    "Responde SI o NO.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    sufficient = \"SI\" in response.content.upper()\n",
    "    return {**state, \"context_sufficient\": sufficient}\n",
    "\n",
    "def generate_response(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Generate response from context.\"\"\"\n",
    "    prompt = f\"\"\"Responde la pregunta basÃ¡ndote SOLO en el contexto proporcionado.\n",
    "Si no hay informaciÃ³n suficiente, indÃ­calo claramente.\n",
    "\n",
    "Contexto:\n",
    "{state['context']}\n",
    "\n",
    "Pregunta: {state['question']}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"response\": response.content}\n",
    "\n",
    "def check_quality(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Check response quality.\"\"\"\n",
    "    prompt = f\"\"\"EvalÃºa la calidad de esta respuesta.\n",
    "    \n",
    "Pregunta: {state['question']}\n",
    "Contexto usado: {state['context']}\n",
    "Respuesta: {state['response']}\n",
    "\n",
    "Â¿La respuesta es BUENA, MEJORABLE o MALA?\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    quality = \"BUENA\" if \"BUENA\" in response.content.upper() else \"MEJORABLE\"\n",
    "    return {**state, \"response_quality\": quality, \"iteration\": state.get(\"iteration\", 0) + 1}\n",
    "\n",
    "def improve_response(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Improve the response.\"\"\"\n",
    "    prompt = f\"\"\"Mejora esta respuesta haciÃ©ndola mÃ¡s clara y completa.\n",
    "    \n",
    "Contexto: {state['context']}\n",
    "Respuesta original: {state['response']}\n",
    "\n",
    "Respuesta mejorada:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"response\": response.content}\n",
    "\n",
    "def finalize(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Finalize the response.\"\"\"\n",
    "    return {**state, \"final_response\": state[\"response\"]}\n",
    "\n",
    "def no_info_response(state: AgenticRAGState) -> AgenticRAGState:\n",
    "    \"\"\"Response when no info found.\"\"\"\n",
    "    return {**state, \"final_response\": \"Lo siento, no tengo informaciÃ³n suficiente para responder a esa pregunta.\"}\n",
    "\n",
    "# Routing functions\n",
    "def route_context(state: AgenticRAGState) -> Literal[\"generate\", \"no_info\"]:\n",
    "    return \"generate\" if state[\"context_sufficient\"] else \"no_info\"\n",
    "\n",
    "def route_quality(state: AgenticRAGState) -> Literal[\"improve\", \"finalize\"]:\n",
    "    if state[\"response_quality\"] != \"BUENA\" and state.get(\"iteration\", 0) < 2:\n",
    "        return \"improve\"\n",
    "    return \"finalize\"\n",
    "\n",
    "print(\"Funciones de nodo definidas âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(AgenticRAGState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify\", classify_question)\n",
    "workflow.add_node(\"reformulate\", reformulate_question)\n",
    "workflow.add_node(\"retrieve\", retrieve_context)\n",
    "workflow.add_node(\"check_context\", check_context)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "workflow.add_node(\"check_quality\", check_quality)\n",
    "workflow.add_node(\"improve\", improve_response)\n",
    "workflow.add_node(\"finalize\", finalize)\n",
    "workflow.add_node(\"no_info\", no_info_response)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"classify\")\n",
    "workflow.add_edge(\"classify\", \"reformulate\")\n",
    "workflow.add_edge(\"reformulate\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"check_context\")\n",
    "workflow.add_conditional_edges(\"check_context\", route_context, \n",
    "                                {\"generate\": \"generate\", \"no_info\": \"no_info\"})\n",
    "workflow.add_edge(\"generate\", \"check_quality\")\n",
    "workflow.add_conditional_edges(\"check_quality\", route_quality,\n",
    "                                {\"improve\": \"improve\", \"finalize\": \"finalize\"})\n",
    "workflow.add_edge(\"improve\", \"check_quality\")\n",
    "workflow.add_edge(\"finalize\", END)\n",
    "workflow.add_edge(\"no_info\", END)\n",
    "\n",
    "# Compile with memory\n",
    "memory = MemorySaver()\n",
    "agentic_rag = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"Agentic RAG compilado âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system\n",
    "def ask(question: str, session_id: str = \"default\"):\n",
    "    config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "    result = agentic_rag.invoke({\n",
    "        \"question\": question,\n",
    "        \"question_type\": \"\",\n",
    "        \"reformulated_question\": \"\",\n",
    "        \"context\": \"\",\n",
    "        \"context_sufficient\": False,\n",
    "        \"response\": \"\",\n",
    "        \"response_quality\": \"\",\n",
    "        \"final_response\": \"\",\n",
    "        \"iteration\": 0\n",
    "    }, config=config)\n",
    "    \n",
    "    print(f\"\\nğŸ“ Pregunta: {question}\")\n",
    "    print(f\"ğŸ·ï¸ Tipo: {result['question_type']}\")\n",
    "    print(f\"ğŸ”„ Reformulada: {result['reformulated_question']}\")\n",
    "    print(f\"âœ… Contexto suficiente: {result['context_sufficient']}\")\n",
    "    print(f\"â­ Calidad: {result['response_quality']}\")\n",
    "    print(f\"ğŸ”¢ Iteraciones: {result['iteration']}\")\n",
    "    print(f\"\\nğŸ’¬ Respuesta final:\\n{result['final_response']}\")\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "ask(\"Â¿QuÃ© bonificaciones hay para vehÃ­culos elÃ©ctricos?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test more questions\n",
    "questions = [\n",
    "    \"Â¿CuÃ¡ndo se paga el IBI?\",\n",
    "    \"Â¿Puedo recurrir una liquidaciÃ³n?\",\n",
    "    \"Â¿QuÃ© descuentos hay para familias numerosas?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    ask(q)\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"evaluacion\"></a>\n",
    "## 4. EvaluaciÃ³n con mÃ©tricas\n",
    "\n",
    "Evaluamos la calidad del sistema RAG con mÃ©tricas clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple evaluation without RAGAS (to avoid dependency issues)\n",
    "def evaluate_response(question, response, context, ground_truth=None):\n",
    "    \"\"\"Simple evaluation of RAG response quality.\"\"\"\n",
    "    \n",
    "    # Check if response is grounded in context (faithfulness)\n",
    "    faithfulness_prompt = f\"\"\"Â¿La respuesta estÃ¡ basada en el contexto proporcionado?\n",
    "    \n",
    "Contexto: {context}\n",
    "Respuesta: {response}\n",
    "\n",
    "PuntÃºa de 0 a 1 (donde 1 es completamente basada en el contexto).\"\"\"\n",
    "    \n",
    "    faith_score = llm.invoke(faithfulness_prompt)\n",
    "    \n",
    "    # Check relevance\n",
    "    relevance_prompt = f\"\"\"Â¿La respuesta es relevante para la pregunta?\n",
    "    \n",
    "Pregunta: {question}\n",
    "Respuesta: {response}\n",
    "\n",
    "PuntÃºa de 0 a 1.\"\"\"\n",
    "    \n",
    "    rel_score = llm.invoke(relevance_prompt)\n",
    "    \n",
    "    return {\n",
    "        \"faithfulness_eval\": faith_score.content,\n",
    "        \"relevance_eval\": rel_score.content\n",
    "    }\n",
    "\n",
    "# Evaluate one response\n",
    "test_result = ask(\"Â¿QuÃ© es el IBI?\")\n",
    "eval_result = evaluate_response(\n",
    "    test_result[\"question\"],\n",
    "    test_result[\"final_response\"],\n",
    "    test_result[\"context\"]\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š EvaluaciÃ³n:\")\n",
    "print(f\"Fidelidad: {eval_result['faithfulness_eval']}\")\n",
    "print(f\"Relevancia: {eval_result['relevance_eval']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"produccion\"></a>\n",
    "## 5. Mejores prÃ¡cticas de producciÃ³n\n",
    "\n",
    "### Checklist para producciÃ³n\n",
    "\n",
    "- [ ] **Logging**: Registrar todas las consultas y respuestas\n",
    "- [ ] **MÃ©tricas**: Monitorear latencia, calidad, uso\n",
    "- [ ] **Rate limiting**: Controlar costos de API\n",
    "- [ ] **Fallbacks**: Respuestas por defecto si falla\n",
    "- [ ] **Caching**: Cachear respuestas frecuentes\n",
    "- [ ] **Testing**: Tests automatizados de regresiÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add logging wrapper\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"AgenticRAG\")\n",
    "\n",
    "def ask_with_logging(question: str, session_id: str = \"default\"):\n",
    "    \"\"\"Wrapper with logging for production.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Query received: {question}\")\n",
    "    \n",
    "    try:\n",
    "        result = ask(question, session_id)\n",
    "        \n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        logger.info(f\"Response generated in {elapsed:.2f}s\")\n",
    "        logger.info(f\"Quality: {result['response_quality']}\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query: {e}\")\n",
    "        return {\"final_response\": \"Lo siento, ocurriÃ³ un error. Intenta de nuevo.\"}\n",
    "\n",
    "# Test\n",
    "ask_with_logging(\"Â¿CuÃ¡les son los plazos de pago?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicio\"></a>\n",
    "## 6. Ejercicio Final\n",
    "\n",
    "### Mejora el sistema Agentic RAG\n",
    "\n",
    "AÃ±ade las siguientes mejoras:\n",
    "1. CitaciÃ³n de fuentes en la respuesta\n",
    "2. DetecciÃ³n de idioma\n",
    "3. Historial de conversaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement improvements\n",
    "# 1. Modify generate_response to include source citations\n",
    "# 2. Add a language detection node\n",
    "# 3. Implement conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos construido un sistema **Agentic RAG** completo:\n",
    "\n",
    "1. **ClasificaciÃ³n** de preguntas\n",
    "2. **ReformulaciÃ³n** para mejor retrieval\n",
    "3. **VerificaciÃ³n** de contexto suficiente\n",
    "4. **GeneraciÃ³n** de respuestas\n",
    "5. **Auto-correcciÃ³n** iterativa\n",
    "6. **EvaluaciÃ³n** de calidad\n",
    "\n",
    "Este patrÃ³n es la base para asistentes de IA en producciÃ³n.\n",
    "\n",
    "En el siguiente y Ãºltimo notebook veremos **Modelos de Gran Contexto y Multimodales**.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [LangGraph Agentic RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)\n",
    "- [RAGAS Evaluation](https://docs.ragas.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
