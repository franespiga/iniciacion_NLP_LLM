{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Reranking y Optimizaci√≥n de Retrieval\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duraci√≥n estimada:** 2 horas\n",
    "\n",
    "---\n",
    "\n",
    "## √çndice\n",
    "\n",
    "1. [Introducci√≥n al Reranking](#intro)\n",
    "2. [Cross-Encoder](#cross)\n",
    "3. [Bi-Encoder (Dense Retrieval)](#bi)\n",
    "4. [LLM como Reranker](#llm)\n",
    "5. [Estrategias combinadas](#combinadas)\n",
    "6. [Ejercicios pr√°cticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, ser√°s capaz de:\n",
    "- Entender por qu√© el reranking mejora la calidad del RAG\n",
    "- Implementar Cross-Encoder para reranking preciso\n",
    "- Comparar Bi-Encoder vs Cross-Encoder\n",
    "- Combinar m√∫ltiples estrategias de retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Introducci√≥n al Reranking\n",
    "\n",
    "### ¬øPor qu√© necesitamos Reranking?\n",
    "\n",
    "La primera fase de retrieval (Bi-Encoder + similitud coseno) es **r√°pida pero imprecisa**. El reranking a√±ade una segunda fase m√°s precisa pero m√°s costosa.\n",
    "\n",
    "```\n",
    "Query ‚Üí [Bi-Encoder] ‚Üí Top 50 candidatos ‚Üí [Reranker] ‚Üí Top 5 finales\n",
    "         (r√°pido)                           (preciso)\n",
    "```\n",
    "\n",
    "### Tipos de Rerankers\n",
    "\n",
    "| Tipo | Descripci√≥n | Precisi√≥n | Velocidad |\n",
    "|------|-------------|-----------|----------|\n",
    "| **Cross-Encoder** | Eval√∫a (query, doc) juntos | Alta | Lenta |\n",
    "| **Bi-Encoder** | Embeddings separados | Media | R√°pida |\n",
    "| **LLM Reranker** | LLM eval√∫a relevancia | Muy alta | Muy lenta |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (all free)\n",
    "!pip install -q sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sample documents for testing\n",
    "query = \"¬øCu√°les son las aplicaciones m√°s prometedoras de la IA en medicina?\"\n",
    "\n",
    "documents = [\n",
    "    \"La IA puede acelerar el descubrimiento de nuevos medicamentos para diversos tipos de c√°ncer.\",\n",
    "    \"En cardiolog√≠a, los modelos predictivos ayudan a identificar pacientes con alto riesgo de infarto.\",\n",
    "    \"Los sistemas de IA permiten analizar im√°genes m√©dicas para detectar tumores en etapas tempranas.\",\n",
    "    \"La IA ha demostrado ser √∫til en el monitoreo de enfermedades cr√≥nicas como la diabetes.\",\n",
    "    \"La IA se utiliza para personalizar tratamientos oncol√≥gicos seg√∫n el perfil gen√©tico.\",\n",
    "    \"En neurolog√≠a, se emplea IA para detectar patrones de deterioro cognitivo.\",\n",
    "    \"Algoritmos se han integrado en hospitales para optimizar la gesti√≥n de camas.\",\n",
    "    \"La IA permite predecir respuestas a inmunoterapia en pacientes con melanoma.\",\n",
    "    \"Me gusta el helado de chocolate.\",  # Irrelevant\n",
    "    \"El tiempo est√° muy agradable hoy.\",  # Irrelevant\n",
    "]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Documentos: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cross\"></a>\n",
    "## 2. Cross-Encoder\n",
    "\n",
    "El **Cross-Encoder** procesa query y documento **juntos** a trav√©s del modelo, permitiendo atenci√≥n cruzada entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load Cross-Encoder model (free, open source)\n",
    "print(\"Cargando Cross-Encoder...\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Modelo cargado ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (query, document) pairs\n",
    "pairs = [(query, doc) for doc in documents]\n",
    "\n",
    "# Get relevance scores\n",
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "# Sort by score\n",
    "results = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüî¥ Reranking con Cross-Encoder:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    emoji = \"‚úÖ\" if score > 0 else \"‚ùå\"\n",
    "    print(f\"{i}. {emoji} Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:70]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bi\"></a>\n",
    "## 3. Bi-Encoder (Dense Retrieval)\n",
    "\n",
    "El **Bi-Encoder** genera embeddings **separados** para query y documentos, luego calcula similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load Bi-Encoder model\n",
    "print(\"Cargando Bi-Encoder...\")\n",
    "bi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Modelo cargado ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode query and documents separately\n",
    "query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "doc_embeddings = bi_encoder.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Sort by similarity\n",
    "results_bi = sorted(zip(documents, similarities.tolist()), \n",
    "                    key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüîµ Ranking con Bi-Encoder:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (doc, score) in enumerate(results_bi, 1):\n",
    "    emoji = \"‚úÖ\" if score > 0.3 else \"‚ùå\"\n",
    "    print(f\"{i}. {emoji} Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:70]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaci√≥n: Cross-Encoder vs Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark speed\n",
    "n_docs = 100\n",
    "test_docs = documents * 10  # Create 100 documents\n",
    "\n",
    "# Bi-Encoder timing\n",
    "start = time.time()\n",
    "q_emb = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "d_embs = bi_encoder.encode(test_docs, convert_to_tensor=True)\n",
    "sims = util.cos_sim(q_emb, d_embs)\n",
    "bi_time = time.time() - start\n",
    "\n",
    "# Cross-Encoder timing\n",
    "start = time.time()\n",
    "pairs = [(query, doc) for doc in test_docs]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "cross_time = time.time() - start\n",
    "\n",
    "print(\"Comparaci√≥n de velocidad:\")\n",
    "print(f\"  Bi-Encoder:    {bi_time:.3f}s para {n_docs} docs\")\n",
    "print(f\"  Cross-Encoder: {cross_time:.3f}s para {n_docs} docs\")\n",
    "print(f\"  Cross-Encoder es {cross_time/bi_time:.1f}x m√°s lento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm\"></a>\n",
    "## 4. LLM como Reranker\n",
    "\n",
    "Un LLM puede evaluar la relevancia de documentos con alta precisi√≥n, pero es el m√©todo m√°s costoso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'GROQ_API_KEY' not in os.environ:\n",
    "    os.environ['GROQ_API_KEY'] = getpass(\"Introduce tu GROQ API Key: \")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "print(\"LLM configurado ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_rerank(query: str, documents: list, top_k: int = 5) -> list:\n",
    "    \"\"\"Use LLM to rerank documents by relevance.\"\"\"\n",
    "    \n",
    "    # Format documents with indices\n",
    "    doc_list = \"\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    prompt = f\"\"\"Ordena los siguientes documentos por relevancia para la pregunta.\n",
    "Devuelve SOLO los n√∫meros de los {top_k} documentos m√°s relevantes, separados por comas.\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Documentos:\n",
    "{doc_list}\n",
    "\n",
    "Top {top_k} m√°s relevantes (solo n√∫meros, ej: 3,1,5,2,4):\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse response\n",
    "    try:\n",
    "        indices = [int(x.strip()) - 1 for x in response.content.split(\",\")[:top_k]]\n",
    "        return [(documents[i], i+1) for i in indices if 0 <= i < len(documents)]\n",
    "    except:\n",
    "        return [(doc, i) for i, doc in enumerate(documents[:top_k], 1)]\n",
    "\n",
    "# Test LLM reranking\n",
    "print(\"üü£ Reranking con LLM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "llm_results = llm_rerank(query, documents, top_k=5)\n",
    "for i, (doc, original_idx) in enumerate(llm_results, 1):\n",
    "    print(f\"{i}. (original #{original_idx}) {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"combinadas\"></a>\n",
    "## 5. Estrategias Combinadas\n",
    "\n",
    "La mejor pr√°ctica es combinar Bi-Encoder (r√°pido, primera fase) con Cross-Encoder (preciso, segunda fase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_stage_retrieval(query: str, documents: list, \n",
    "                        bi_model, cross_model,\n",
    "                        first_k: int = 10, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Two-stage retrieval:\n",
    "    1. Bi-Encoder for fast initial retrieval\n",
    "    2. Cross-Encoder for precise reranking\n",
    "    \"\"\"\n",
    "    # Stage 1: Bi-Encoder\n",
    "    q_emb = bi_model.encode(query, convert_to_tensor=True)\n",
    "    d_embs = bi_model.encode(documents, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(q_emb, d_embs)[0]\n",
    "    \n",
    "    # Get top-k candidates\n",
    "    top_indices = sims.argsort(descending=True)[:first_k]\n",
    "    candidates = [(documents[i], sims[i].item()) for i in top_indices]\n",
    "    \n",
    "    print(f\"Stage 1 (Bi-Encoder): {len(candidates)} candidatos\")\n",
    "    \n",
    "    # Stage 2: Cross-Encoder\n",
    "    pairs = [(query, doc) for doc, _ in candidates]\n",
    "    scores = cross_model.predict(pairs)\n",
    "    \n",
    "    # Final ranking\n",
    "    final_results = sorted(zip([c[0] for c in candidates], scores), \n",
    "                          key=lambda x: x[1], reverse=True)[:final_k]\n",
    "    \n",
    "    print(f\"Stage 2 (Cross-Encoder): {len(final_results)} resultados finales\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# Test two-stage retrieval\n",
    "print(\"\\nüî∂ Two-Stage Retrieval:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_results = two_stage_retrieval(\n",
    "    query, documents,\n",
    "    bi_encoder, cross_encoder,\n",
    "    first_k=8, final_k=5\n",
    ")\n",
    "\n",
    "print(\"\\nResultados finales:\")\n",
    "for i, (doc, score) in enumerate(final_results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:70]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 6. Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Comparar rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compare all three methods on a new query\n",
    "\n",
    "new_query = \"¬øC√≥mo ayuda la inteligencia artificial a diagnosticar enfermedades?\"\n",
    "\n",
    "# Get rankings from all three methods\n",
    "# 1. Bi-Encoder\n",
    "# 2. Cross-Encoder  \n",
    "# 3. LLM\n",
    "\n",
    "# Compare which documents appear in top 3 for each\n",
    "# Are they the same? Different?\n",
    "\n",
    "print(f\"Query: {new_query}\")\n",
    "print(\"\\nCompara los rankings de cada m√©todo...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **Reranking**: Segunda fase para mejorar precisi√≥n\n",
    "2. **Cross-Encoder**: Preciso pero lento\n",
    "3. **Bi-Encoder**: R√°pido pero menos preciso\n",
    "4. **LLM Reranker**: Muy preciso pero costoso\n",
    "5. **Two-stage**: Combinar lo mejor de ambos\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "| Escenario | Estrategia recomendada |\n",
    "|-----------|------------------------|\n",
    "| Latencia cr√≠tica | Solo Bi-Encoder |\n",
    "| Precisi√≥n cr√≠tica | Two-stage (Bi + Cross) |\n",
    "| Pocos documentos | Cross-Encoder directo |\n",
    "| M√°xima precisi√≥n | LLM Reranker |\n",
    "\n",
    "En el siguiente notebook veremos **Agentes**, que combinan LLMs con herramientas y razonamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Cross-Encoders for Reranking](https://www.sbert.net/examples/applications/cross-encoder/README.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
