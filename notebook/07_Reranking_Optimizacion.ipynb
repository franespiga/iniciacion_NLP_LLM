{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Reranking y Optimizaci√≥n de Retrieval\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duraci√≥n estimada:** 2 horas\n",
    "\n",
    "---\n",
    "\n",
    "## √çndice\n",
    "\n",
    "1. [Introducci√≥n al Reranking](#intro)\n",
    "2. [Cross-Encoder](#cross)\n",
    "3. [Bi-Encoder (Dense Retrieval)](#bi)\n",
    "4. [LLM como Reranker](#llm)\n",
    "5. [Estrategias combinadas](#combinadas)\n",
    "6. [Ejercicios pr√°cticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, ser√°s capaz de:\n",
    "- Entender por qu√© el reranking mejora la calidad del RAG\n",
    "- Implementar Cross-Encoder para reranking preciso\n",
    "- Comparar Bi-Encoder vs Cross-Encoder\n",
    "- Combinar m√∫ltiples estrategias de retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Introducci√≥n al Reranking\n",
    "\n",
    "### ¬øPor qu√© necesitamos Reranking?\n",
    "\n",
    "La primera fase de retrieval (Bi-Encoder + similitud coseno) es **r√°pida pero imprecisa**. El reranking a√±ade una segunda fase m√°s precisa pero m√°s costosa.\n",
    "\n",
    "```\n",
    "Query ‚Üí [Bi-Encoder] ‚Üí Top 50 candidatos ‚Üí [Reranker] ‚Üí Top 5 finales\n",
    "         (r√°pido)                           (preciso)\n",
    "```\n",
    "\n",
    "### Tipos de Rerankers\n",
    "\n",
    "| Tipo | Descripci√≥n | Precisi√≥n | Velocidad |\n",
    "|------|-------------|-----------|----------|\n",
    "| **Cross-Encoder** | Eval√∫a (query, doc) juntos | Alta | Lenta |\n",
    "| **Bi-Encoder** | Embeddings separados | Media | R√°pida |\n",
    "| **LLM Reranker** | LLM eval√∫a relevancia | Muy alta | Muy lenta |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (all free)\n",
    "#!pip install -q sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ¬øCu√°les son las aplicaciones m√°s prometedoras de la IA en medicina?\n",
      "Documentos: 10\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sample documents for testing\n",
    "query = \"¬øCu√°les son las aplicaciones m√°s prometedoras de la IA en medicina?\"\n",
    "\n",
    "documents = [\n",
    "    \"La IA puede acelerar el descubrimiento de nuevos medicamentos para diversos tipos de c√°ncer.\",\n",
    "    \"En cardiolog√≠a, los modelos predictivos ayudan a identificar pacientes con alto riesgo de infarto.\",\n",
    "    \"Los sistemas de IA permiten analizar im√°genes m√©dicas para detectar tumores en etapas tempranas.\",\n",
    "    \"La IA ha demostrado ser √∫til en el monitoreo de enfermedades cr√≥nicas como la diabetes.\",\n",
    "    \"La IA se utiliza para personalizar tratamientos oncol√≥gicos seg√∫n el perfil gen√©tico.\",\n",
    "    \"En neurolog√≠a, se emplea IA para detectar patrones de deterioro cognitivo.\",\n",
    "    \"Algoritmos se han integrado en hospitales para optimizar la gesti√≥n de camas.\",\n",
    "    \"La IA permite predecir respuestas a inmunoterapia en pacientes con melanoma.\",\n",
    "    \"Me gusta el helado de chocolate.\",  # Irrelevant\n",
    "    \"El tiempo est√° muy agradable hoy.\",  # Irrelevant\n",
    "]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Documentos: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cross\"></a>\n",
    "## 2. Cross-Encoder\n",
    "\n",
    "El **Cross-Encoder** procesa query y documento **juntos** a trav√©s del modelo, permitiendo atenci√≥n cruzada entre ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Cross-Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:00<00:00, 1096.17it/s, Materializing param=classifier.weight]\n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado ‚úì\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load Cross-Encoder model (free, open source)\n",
    "print(\"Cargando Cross-Encoder...\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Modelo cargado ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¥ Reranking con Cross-Encoder:\n",
      "============================================================\n",
      "1. ‚ùå Score: -9.9813\n",
      "   La IA se utiliza para personalizar tratamientos oncol√≥gicos seg√∫n el p...\n",
      "\n",
      "2. ‚ùå Score: -10.0438\n",
      "   La IA ha demostrado ser √∫til en el monitoreo de enfermedades cr√≥nicas ...\n",
      "\n",
      "3. ‚ùå Score: -10.1135\n",
      "   La IA permite predecir respuestas a inmunoterapia en pacientes con mel...\n",
      "\n",
      "4. ‚ùå Score: -10.1852\n",
      "   La IA puede acelerar el descubrimiento de nuevos medicamentos para div...\n",
      "\n",
      "5. ‚ùå Score: -10.3119\n",
      "   El tiempo est√° muy agradable hoy....\n",
      "\n",
      "6. ‚ùå Score: -10.7006\n",
      "   Los sistemas de IA permiten analizar im√°genes m√©dicas para detectar tu...\n",
      "\n",
      "7. ‚ùå Score: -10.8199\n",
      "   Algoritmos se han integrado en hospitales para optimizar la gesti√≥n de...\n",
      "\n",
      "8. ‚ùå Score: -10.8908\n",
      "   En neurolog√≠a, se emplea IA para detectar patrones de deterioro cognit...\n",
      "\n",
      "9. ‚ùå Score: -10.9315\n",
      "   En cardiolog√≠a, los modelos predictivos ayudan a identificar pacientes...\n",
      "\n",
      "10. ‚ùå Score: -10.9830\n",
      "   Me gusta el helado de chocolate....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create (query, document) pairs\n",
    "pairs = [(query, doc) for doc in documents]\n",
    "\n",
    "# Get relevance scores\n",
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "# Sort by score\n",
    "results = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüî¥ Reranking con Cross-Encoder:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    emoji = \"‚úÖ\" if score > 0 else \"‚ùå\"\n",
    "    print(f\"{i}. {emoji} Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:70]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"bi\"></a>\n",
    "## 3. Bi-Encoder (Dense Retrieval)\n",
    "\n",
    "El **Bi-Encoder** genera embeddings **separados** para query y documentos, luego calcula similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Bi-Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1112.50it/s, Materializing param=pooler.dense.weight]\n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado ‚úì\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load Bi-Encoder model\n",
    "print(\"Cargando Bi-Encoder...\")\n",
    "bi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Modelo cargado ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ Ranking con Bi-Encoder:\n",
      "============================================================\n",
      "1. ‚úÖ Score: 0.5146\n",
      "   Algoritmos se han integrado en hospitales para optimizar la gesti√≥n de...\n",
      "\n",
      "2. ‚úÖ Score: 0.4947\n",
      "   La IA se utiliza para personalizar tratamientos oncol√≥gicos seg√∫n el p...\n",
      "\n",
      "3. ‚úÖ Score: 0.4938\n",
      "   La IA puede acelerar el descubrimiento de nuevos medicamentos para div...\n",
      "\n",
      "4. ‚úÖ Score: 0.4393\n",
      "   La IA ha demostrado ser √∫til en el monitoreo de enfermedades cr√≥nicas ...\n",
      "\n",
      "5. ‚úÖ Score: 0.4010\n",
      "   El tiempo est√° muy agradable hoy....\n",
      "\n",
      "6. ‚úÖ Score: 0.3870\n",
      "   Me gusta el helado de chocolate....\n",
      "\n",
      "7. ‚úÖ Score: 0.3823\n",
      "   En neurolog√≠a, se emplea IA para detectar patrones de deterioro cognit...\n",
      "\n",
      "8. ‚úÖ Score: 0.3372\n",
      "   La IA permite predecir respuestas a inmunoterapia en pacientes con mel...\n",
      "\n",
      "9. ‚úÖ Score: 0.3301\n",
      "   Los sistemas de IA permiten analizar im√°genes m√©dicas para detectar tu...\n",
      "\n",
      "10. ‚ùå Score: 0.2693\n",
      "   En cardiolog√≠a, los modelos predictivos ayudan a identificar pacientes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode query and documents separately\n",
    "query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "doc_embeddings = bi_encoder.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Sort by similarity\n",
    "results_bi = sorted(zip(documents, similarities.tolist()), \n",
    "                    key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüîµ Ranking con Bi-Encoder:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (doc, score) in enumerate(results_bi, 1):\n",
    "    emoji = \"‚úÖ\" if score > 0.3 else \"‚ùå\"\n",
    "    print(f\"{i}. {emoji} Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:70]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaci√≥n: Cross-Encoder vs Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaci√≥n de velocidad:\n",
      "  Bi-Encoder:    0.241s para 100 docs\n",
      "  Cross-Encoder: 0.397s para 100 docs\n",
      "  Cross-Encoder es 1.6x m√°s lento\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark speed\n",
    "n_docs = 100\n",
    "test_docs = documents * 10  # Create 100 documents\n",
    "\n",
    "# Bi-Encoder timing\n",
    "start = time.time()\n",
    "q_emb = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "d_embs = bi_encoder.encode(test_docs, convert_to_tensor=True)\n",
    "sims = util.cos_sim(q_emb, d_embs)\n",
    "bi_time = time.time() - start\n",
    "\n",
    "# Cross-Encoder timing\n",
    "start = time.time()\n",
    "pairs = [(query, doc) for doc in test_docs]\n",
    "scores = cross_encoder.predict(pairs)\n",
    "cross_time = time.time() - start\n",
    "\n",
    "print(\"Comparaci√≥n de velocidad:\")\n",
    "print(f\"  Bi-Encoder:    {bi_time:.3f}s para {n_docs} docs\")\n",
    "print(f\"  Cross-Encoder: {cross_time:.3f}s para {n_docs} docs\")\n",
    "print(f\"  Cross-Encoder es {cross_time/bi_time:.1f}x m√°s lento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm\"></a>\n",
    "## 4. LLM como Reranker\n",
    "\n",
    "Un LLM puede evaluar la relevancia de documentos con alta precisi√≥n, pero es el m√©todo m√°s costoso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu GROQ API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM configurado ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if 'GROQ_API_KEY' not in os.environ:\n",
    "    os.environ['GROQ_API_KEY'] = getpass(\"Introduce tu GROQ API Key: \")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "print(\"LLM configurado ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü£ Reranking con LLM:\n",
      "============================================================\n",
      "1. (original #1) La IA puede acelerar el descubrimiento de nuevos medicamento...\n",
      "2. (original #3) Los sistemas de IA permiten analizar im√°genes m√©dicas para d...\n",
      "3. (original #5) La IA se utiliza para personalizar tratamientos oncol√≥gicos ...\n",
      "4. (original #2) En cardiolog√≠a, los modelos predictivos ayudan a identificar...\n",
      "5. (original #4) La IA ha demostrado ser √∫til en el monitoreo de enfermedades...\n"
     ]
    }
   ],
   "source": [
    "def llm_rerank(query: str, documents: list, top_k: int = 5) -> list:\n",
    "    \"\"\"Use LLM to rerank documents by relevance.\"\"\"\n",
    "    \n",
    "    # Format documents with indices\n",
    "    doc_list = \"\\n\".join([f\"[{i+1}] {doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    prompt = f\"\"\"Ordena los siguientes documentos por relevancia para la pregunta.\n",
    "Devuelve SOLO los n√∫meros de los {top_k} documentos m√°s relevantes, separados por comas.\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Documentos:\n",
    "{doc_list}\n",
    "\n",
    "Top {top_k} m√°s relevantes (solo n√∫meros, ej: 3,1,5,2,4):\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    # Parse response\n",
    "    try:\n",
    "        indices = [int(x.strip()) - 1 for x in response.content.split(\",\")[:top_k]]\n",
    "        return [(documents[i], i+1) for i in indices if 0 <= i < len(documents)]\n",
    "    except:\n",
    "        return [(doc, i) for i, doc in enumerate(documents[:top_k], 1)]\n",
    "\n",
    "# Test LLM reranking\n",
    "print(\"üü£ Reranking con LLM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "llm_results = llm_rerank(query, documents, top_k=5)\n",
    "for i, (doc, original_idx) in enumerate(llm_results, 1):\n",
    "    print(f\"{i}. (original #{original_idx}) {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"combinadas\"></a>\n",
    "## 5. Estrategias Combinadas\n",
    "\n",
    "La mejor pr√°ctica es combinar Bi-Encoder (r√°pido, primera fase) con Cross-Encoder (preciso, segunda fase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî∂ Two-Stage Retrieval:\n",
      "============================================================\n",
      "Stage 1 (Bi-Encoder): 8 candidatos\n",
      "Stage 2 (Cross-Encoder): 5 resultados finales\n",
      "\n",
      "Resultados finales:\n",
      "1. Score: -9.9813\n",
      "   La IA se utiliza para personalizar tratamientos oncol√≥gicos seg√∫n el p...\n",
      "\n",
      "2. Score: -10.0438\n",
      "   La IA ha demostrado ser √∫til en el monitoreo de enfermedades cr√≥nicas ...\n",
      "\n",
      "3. Score: -10.1135\n",
      "   La IA permite predecir respuestas a inmunoterapia en pacientes con mel...\n",
      "\n",
      "4. Score: -10.1852\n",
      "   La IA puede acelerar el descubrimiento de nuevos medicamentos para div...\n",
      "\n",
      "5. Score: -10.3119\n",
      "   El tiempo est√° muy agradable hoy....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def two_stage_retrieval(query: str, documents: list, \n",
    "                        bi_model, cross_model,\n",
    "                        first_k: int = 10, final_k: int = 5):\n",
    "    \"\"\"\n",
    "    Two-stage retrieval:\n",
    "    1. Bi-Encoder for fast initial retrieval\n",
    "    2. Cross-Encoder for precise reranking\n",
    "    \"\"\"\n",
    "    # Stage 1: Bi-Encoder\n",
    "    q_emb = bi_model.encode(query, convert_to_tensor=True)\n",
    "    d_embs = bi_model.encode(documents, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(q_emb, d_embs)[0]\n",
    "    \n",
    "    # Get top-k candidates\n",
    "    top_indices = sims.argsort(descending=True)[:first_k]\n",
    "    candidates = [(documents[i], sims[i].item()) for i in top_indices]\n",
    "    \n",
    "    print(f\"Stage 1 (Bi-Encoder): {len(candidates)} candidatos\")\n",
    "    \n",
    "    # Stage 2: Cross-Encoder\n",
    "    pairs = [(query, doc) for doc, _ in candidates]\n",
    "    scores = cross_model.predict(pairs)\n",
    "    \n",
    "    # Final ranking\n",
    "    final_results = sorted(zip([c[0] for c in candidates], scores), \n",
    "                          key=lambda x: x[1], reverse=True)[:final_k]\n",
    "    \n",
    "    print(f\"Stage 2 (Cross-Encoder): {len(final_results)} resultados finales\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# Test two-stage retrieval\n",
    "print(\"\\nüî∂ Two-Stage Retrieval:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_results = two_stage_retrieval(\n",
    "    query, documents,\n",
    "    bi_encoder, cross_encoder,\n",
    "    first_k=8, final_k=5\n",
    ")\n",
    "\n",
    "print(\"\\nResultados finales:\")\n",
    "for i, (doc, score) in enumerate(final_results, 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:70]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 6. Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Comparar rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ¬øC√≥mo ayuda la inteligencia artificial a diagnosticar enfermedades?\n",
      "\n",
      "Compara los rankings de cada m√©todo...\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Compare all three methods on a new query\n",
    "\n",
    "new_query = \"¬øC√≥mo ayuda la inteligencia artificial a diagnosticar enfermedades?\"\n",
    "\n",
    "# Get rankings from all three methods\n",
    "# 1. Bi-Encoder\n",
    "# 2. Cross-Encoder  \n",
    "# 3. LLM\n",
    "\n",
    "# Compare which documents appear in top 3 for each\n",
    "# Are they the same? Different?\n",
    "\n",
    "print(f\"Query: {new_query}\")\n",
    "print(\"\\nCompara los rankings de cada m√©todo...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **Reranking**: Segunda fase para mejorar precisi√≥n\n",
    "2. **Cross-Encoder**: Preciso pero lento\n",
    "3. **Bi-Encoder**: R√°pido pero menos preciso\n",
    "4. **LLM Reranker**: Muy preciso pero costoso\n",
    "5. **Two-stage**: Combinar lo mejor de ambos\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "| Escenario | Estrategia recomendada |\n",
    "|-----------|------------------------|\n",
    "| Latencia cr√≠tica | Solo Bi-Encoder |\n",
    "| Precisi√≥n cr√≠tica | Two-stage (Bi + Cross) |\n",
    "| Pocos documentos | Cross-Encoder directo |\n",
    "| M√°xima precisi√≥n | LLM Reranker |\n",
    "\n",
    "En el siguiente notebook veremos **Agentes**, que combinan LLMs con herramientas y razonamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Cross-Encoders for Reranking](https://www.sbert.net/examples/applications/cross-encoder/README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "ipykernel                   7.2.0\n",
      "langchain_groq              1.1.2\n",
      "sentence_transformers       5.2.2\n",
      "session_info                v1.0.1\n",
      "torch                       2.10.0+cpu\n",
      "-----\n",
      "IPython             9.10.0\n",
      "jupyter_client      8.8.0\n",
      "jupyter_core        5.9.1\n",
      "-----\n",
      "Python 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\n",
      "Windows-11-10.0.26200-SP0\n",
      "-----\n",
      "Session information updated at 2026-02-09 17:25\n"
     ]
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show(html = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_LLM",
   "language": "python",
   "name": "ia_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
