{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - LangGraph y Flujos de Trabajo\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duración estimada:** 2.5-3 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Índice\n",
    "\n",
    "1. [Introducción a LangGraph](#intro)\n",
    "2. [Estados y Grafos](#estados)\n",
    "3. [Flujo RAG con LangGraph](#rag)\n",
    "4. [Auto-corrección](#correccion)\n",
    "5. [Checkpoints y Persistencia](#checkpoints)\n",
    "6. [Ejercicios prácticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, serás capaz de:\n",
    "- Crear grafos de estados con LangGraph\n",
    "- Implementar flujos condicionales\n",
    "- Añadir auto-corrección a sistemas RAG\n",
    "- Usar checkpoints para persistencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Introducción a LangGraph\n",
    "\n",
    "**LangGraph** es una librería de LangChain para crear flujos de trabajo como grafos de estados.\n",
    "\n",
    "### ¿Por qué LangGraph?\n",
    "\n",
    "- **Control explícito**: Define exactamente el flujo\n",
    "- **Condicionales**: Diferentes caminos según resultados\n",
    "- **Ciclos**: Permite iteraciones y re-intentos\n",
    "- **Estado**: Mantiene información entre nodos\n",
    "- **Persistencia**: Checkpoints para recuperación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "#!pip install -q langchain langchain-groq langgraph langchain-huggingface faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GROQ API Key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurado ✓\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if 'GROQ_API_KEY' not in os.environ:\n",
    "    os.environ['GROQ_API_KEY'] = getpass(\"GROQ API Key: \")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "print(\"Configurado ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"estados\"></a>\n",
    "## 2. Estados y Grafos\n",
    "\n",
    "En LangGraph, definimos:\n",
    "- **State**: Datos que fluyen por el grafo\n",
    "- **Nodes**: Funciones que procesan el estado\n",
    "- **Edges**: Conexiones entre nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo compilado ✓\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define state\n",
    "class SimpleState(TypedDict):\n",
    "    messages: List[str]\n",
    "    current_step: str\n",
    "\n",
    "# Define nodes\n",
    "def step_one(state: SimpleState) -> SimpleState:\n",
    "    messages = state[\"messages\"] + [\"Paso 1 completado\"]\n",
    "    return {\"messages\": messages, \"current_step\": \"one\"}\n",
    "\n",
    "def step_two(state: SimpleState) -> SimpleState:\n",
    "    messages = state[\"messages\"] + [\"Paso 2 completado\"]\n",
    "    return {\"messages\": messages, \"current_step\": \"two\"}\n",
    "\n",
    "def step_three(state: SimpleState) -> SimpleState:\n",
    "    messages = state[\"messages\"] + [\"Paso 3 completado\"]\n",
    "    return {\"messages\": messages, \"current_step\": \"three\"}\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(SimpleState)\n",
    "workflow.add_node(\"step_one\", step_one)\n",
    "workflow.add_node(\"step_two\", step_two)\n",
    "workflow.add_node(\"step_three\", step_three)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"step_one\")\n",
    "workflow.add_edge(\"step_one\", \"step_two\")\n",
    "workflow.add_edge(\"step_two\", \"step_three\")\n",
    "workflow.add_edge(\"step_three\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "print(\"Grafo compilado ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado:\n",
      "  - Inicio\n",
      "  - Paso 1 completado\n",
      "  - Paso 2 completado\n",
      "  - Paso 3 completado\n"
     ]
    }
   ],
   "source": [
    "# Run the graph\n",
    "result = app.invoke({\"messages\": [\"Inicio\"], \"current_step\": \"\"})\n",
    "\n",
    "print(\"Resultado:\")\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"  - {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafos con condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo condicional compilado ✓\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class ConditionalState(TypedDict):\n",
    "    query: str\n",
    "    query_type: str\n",
    "    response: str\n",
    "\n",
    "def classify_query(state: ConditionalState) -> ConditionalState:\n",
    "    \"\"\"Classify the query type.\"\"\"\n",
    "    query = state[\"query\"].lower()\n",
    "    if \"precio\" in query or \"costo\" in query:\n",
    "        return {**state, \"query_type\": \"pricing\"}\n",
    "    elif \"horario\" in query or \"hora\" in query:\n",
    "        return {**state, \"query_type\": \"schedule\"}\n",
    "    else:\n",
    "        return {**state, \"query_type\": \"general\"}\n",
    "\n",
    "def handle_pricing(state: ConditionalState) -> ConditionalState:\n",
    "    return {**state, \"response\": \"Los precios son: Básico 99€, Pro 299€, Enterprise consultar.\"}\n",
    "\n",
    "def handle_schedule(state: ConditionalState) -> ConditionalState:\n",
    "    return {**state, \"response\": \"Horario: Lunes a Viernes, 9:00 a 18:00.\"}\n",
    "\n",
    "def handle_general(state: ConditionalState) -> ConditionalState:\n",
    "    return {**state, \"response\": \"Para más información, contacta con soporte@empresa.com\"}\n",
    "\n",
    "def route_query(state: ConditionalState) -> Literal[\"pricing\", \"schedule\", \"general\"]:\n",
    "    return state[\"query_type\"]\n",
    "\n",
    "# Build conditional graph\n",
    "cond_workflow = StateGraph(ConditionalState)\n",
    "cond_workflow.add_node(\"classify\", classify_query)\n",
    "cond_workflow.add_node(\"pricing\", handle_pricing)\n",
    "cond_workflow.add_node(\"schedule\", handle_schedule)\n",
    "cond_workflow.add_node(\"general\", handle_general)\n",
    "\n",
    "cond_workflow.add_edge(START, \"classify\")\n",
    "cond_workflow.add_conditional_edges(\n",
    "    \"classify\",\n",
    "    route_query,\n",
    "    {\"pricing\": \"pricing\", \"schedule\": \"schedule\", \"general\": \"general\"}\n",
    ")\n",
    "cond_workflow.add_edge(\"pricing\", END)\n",
    "cond_workflow.add_edge(\"schedule\", END)\n",
    "cond_workflow.add_edge(\"general\", END)\n",
    "\n",
    "cond_app = cond_workflow.compile()\n",
    "print(\"Grafo condicional compilado ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Cuál es el precio del plan básico?\n",
      "A: Los precios son: Básico 99€, Pro 299€, Enterprise consultar.\n",
      "\n",
      "Q: ¿Cuál es el horario de atención?\n",
      "A: Horario: Lunes a Viernes, 9:00 a 18:00.\n",
      "\n",
      "Q: ¿Tienen servicio en México?\n",
      "A: Para más información, contacta con soporte@empresa.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test conditional routing\n",
    "queries = [\n",
    "    \"¿Cuál es el precio del plan básico?\",\n",
    "    \"¿Cuál es el horario de atención?\",\n",
    "    \"¿Tienen servicio en México?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    result = cond_app.invoke({\"query\": q, \"query_type\": \"\", \"response\": \"\"})\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rag\"></a>\n",
    "## 3. Flujo RAG con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█████████████████████████████████████████████████████████████████████| 103/103 [00:00<00:00, 861.33it/s, Materializing param=pooler.dense.weight]\n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store creado ✓\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Create vector store\n",
    "docs = [\n",
    "    Document(page_content=\"El IBI se paga anualmente basado en el valor catastral.\"),\n",
    "    Document(page_content=\"El IVTM grava la titularidad de vehículos matriculados.\"),\n",
    "    Document(page_content=\"El ICIO se liquida al finalizar construcciones u obras.\"),\n",
    "    Document(page_content=\"Las bonificaciones pueden reducir hasta un 90% el impuesto.\"),\n",
    "]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "print(\"Vector store creado ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG graph compilado ✓\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    context: str\n",
    "    response: str\n",
    "\n",
    "def retrieve_context(state: RAGState) -> RAGState:\n",
    "    \"\"\"Retrieve relevant documents.\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    docs = retriever.invoke(query)\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    return {**state, \"context\": context}\n",
    "\n",
    "def generate_response(state: RAGState) -> RAGState:\n",
    "    \"\"\"Generate response using LLM.\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    prompt = f\"\"\"Responde basándote en el contexto.\n",
    "    \n",
    "Contexto: {context}\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"response\": response.content}\n",
    "\n",
    "# Build RAG graph\n",
    "rag_workflow = StateGraph(RAGState)\n",
    "rag_workflow.add_node(\"retrieve\", retrieve_context)\n",
    "rag_workflow.add_node(\"generate\", generate_response)\n",
    "\n",
    "rag_workflow.add_edge(START, \"retrieve\")\n",
    "rag_workflow.add_edge(\"retrieve\", \"generate\")\n",
    "rag_workflow.add_edge(\"generate\", END)\n",
    "\n",
    "rag_app = rag_workflow.compile()\n",
    "print(\"RAG graph compilado ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: El IBI (Impuesto sobre Bienes Inmuebles) es un impuesto que se paga anualmente y está basado en el valor catastral de un inmueble.\n"
     ]
    }
   ],
   "source": [
    "# Test RAG\n",
    "result = rag_app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"¿Qué es el IBI?\")],\n",
    "    \"context\": \"\",\n",
    "    \"response\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"Respuesta: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"correccion\"></a>\n",
    "## 4. Auto-corrección\n",
    "\n",
    "Añadimos un paso de verificación y corrección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo con corrección compilado ✓\n"
     ]
    }
   ],
   "source": [
    "class CorrectionState(TypedDict):\n",
    "    query: str\n",
    "    context: str\n",
    "    response: str\n",
    "    corrected_response: str\n",
    "    needs_correction: bool\n",
    "\n",
    "def retrieve(state: CorrectionState) -> CorrectionState:\n",
    "    docs = retriever.invoke(state[\"query\"])\n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    return {**state, \"context\": context}\n",
    "\n",
    "def generate(state: CorrectionState) -> CorrectionState:\n",
    "    prompt = f\"Contexto: {state['context']}\\nPregunta: {state['query']}\\nRespuesta:\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"response\": response.content}\n",
    "\n",
    "def check_response(state: CorrectionState) -> CorrectionState:\n",
    "    \"\"\"Check if response needs correction.\"\"\"\n",
    "    check_prompt = f\"\"\"¿La siguiente respuesta está basada en el contexto?\n",
    "    \n",
    "Contexto: {state['context']}\n",
    "Respuesta: {state['response']}\n",
    "\n",
    "Responde solo 'SI' o 'NO'.\"\"\"\n",
    "    \n",
    "    check = llm.invoke(check_prompt)\n",
    "    needs_correction = \"NO\" in check.content.upper()\n",
    "    return {**state, \"needs_correction\": needs_correction}\n",
    "\n",
    "def correct_response(state: CorrectionState) -> CorrectionState:\n",
    "    \"\"\"Correct the response.\"\"\"\n",
    "    correct_prompt = f\"\"\"Mejora esta respuesta basándote solo en el contexto.\n",
    "    \n",
    "Contexto: {state['context']}\n",
    "Respuesta original: {state['response']}\n",
    "\n",
    "Respuesta mejorada:\"\"\"\n",
    "    \n",
    "    corrected = llm.invoke(correct_prompt)\n",
    "    return {**state, \"corrected_response\": corrected.content}\n",
    "\n",
    "def route_correction(state: CorrectionState) -> Literal[\"correct\", \"end\"]:\n",
    "    return \"correct\" if state[\"needs_correction\"] else \"end\"\n",
    "\n",
    "# Build correction graph\n",
    "corr_workflow = StateGraph(CorrectionState)\n",
    "corr_workflow.add_node(\"retrieve\", retrieve)\n",
    "corr_workflow.add_node(\"generate\", generate)\n",
    "corr_workflow.add_node(\"check\", check_response)\n",
    "corr_workflow.add_node(\"correct\", correct_response)\n",
    "\n",
    "corr_workflow.add_edge(START, \"retrieve\")\n",
    "corr_workflow.add_edge(\"retrieve\", \"generate\")\n",
    "corr_workflow.add_edge(\"generate\", \"check\")\n",
    "corr_workflow.add_conditional_edges(\"check\", route_correction, {\"correct\": \"correct\", \"end\": END})\n",
    "corr_workflow.add_edge(\"correct\", END)\n",
    "\n",
    "corr_app = corr_workflow.compile()\n",
    "print(\"Grafo con corrección compilado ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta original: El IVTM se paga anualmente, al igual que el IBI, pero se basa en la titularidad de vehículos matriculados en lugar del valor catastral de una propiedad. Por lo general, el plazo para el pago del IVTM varía según la comunidad autónoma o la región en la que se encuentra el vehículo, pero suele ser anual.\n",
      "Necesitó corrección: False\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "result = corr_app.invoke({\n",
    "    \"query\": \"¿Cuándo se paga el IVTM?\",\n",
    "    \"context\": \"\",\n",
    "    \"response\": \"\",\n",
    "    \"corrected_response\": \"\",\n",
    "    \"needs_correction\": False\n",
    "})\n",
    "\n",
    "print(f\"Respuesta original: {result['response']}\")\n",
    "print(f\"Necesitó corrección: {result['needs_correction']}\")\n",
    "if result['corrected_response']:\n",
    "    print(f\"Respuesta corregida: {result['corrected_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"checkpoints\"></a>\n",
    "## 5. Checkpoints y Persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Hay varios impuestos, pero en este contexto, se mencionan específicamente el impuesto que se puede reducir con bonificaciones (hasta un 90%) y el Impuesto sobre Bienes Inmuebles (IBI), que se paga anualmente basado en el valor catastral.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Create checkpointer\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile with checkpointer\n",
    "rag_with_memory = rag_workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Run with thread_id for session tracking\n",
    "config = {\"configurable\": {\"thread_id\": \"session1\"}}\n",
    "\n",
    "result = rag_with_memory.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"¿Qué impuestos hay?\")],\n",
    "    \"context\": \"\",\n",
    "    \"response\": \"\"\n",
    "}, config=config)\n",
    "\n",
    "print(f\"Respuesta: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 6. Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio: Crear un flujo con múltiples pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create a workflow that:\n",
    "# 1. Receives a question\n",
    "# 2. Classifies the question type\n",
    "# 3. Retrieves relevant info\n",
    "# 4. Generates response\n",
    "# 5. Checks quality\n",
    "# 6. Corrects if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **LangGraph**: Crear flujos como grafos de estados\n",
    "2. **Condicionales**: Routing basado en resultados\n",
    "3. **RAG workflow**: Retrieve → Generate\n",
    "4. **Auto-corrección**: Verificar y mejorar respuestas\n",
    "5. **Checkpoints**: Persistencia de sesiones\n",
    "\n",
    "En el siguiente notebook veremos **RAG Avanzado Agentic** con flujos completos.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "ipykernel                   7.2.0\n",
      "langchain_community         0.4.1\n",
      "langchain_core              1.2.9\n",
      "langchain_groq              1.1.2\n",
      "langchain_huggingface       NA\n",
      "langgraph                   NA\n",
      "session_info                v1.0.1\n",
      "-----\n",
      "IPython             9.10.0\n",
      "jupyter_client      8.8.0\n",
      "jupyter_core        5.9.1\n",
      "-----\n",
      "Python 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\n",
      "Windows-11-10.0.26200-SP0\n",
      "-----\n",
      "Session information updated at 2026-02-10 07:07\n"
     ]
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_LLM",
   "language": "python",
   "name": "ia_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
