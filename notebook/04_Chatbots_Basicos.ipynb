{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Chatbots B√°sicos\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**Duraci√≥n estimada:** 1.5-2 horas\n",
    "\n",
    "---\n",
    "\n",
    "## √çndice\n",
    "\n",
    "1. [Introducci√≥n a los Chatbots](#intro)\n",
    "2. [Chatbot con reglas simples](#reglas)\n",
    "3. [Chatbot con LLM (API gratuita)](#llm)\n",
    "4. [A√±adiendo memoria a la conversaci√≥n](#memoria)\n",
    "5. [Interfaces con Streamlit](#streamlit)\n",
    "6. [Ejercicios pr√°cticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, ser√°s capaz de:\n",
    "- Entender los diferentes tipos de chatbots\n",
    "- Construir un chatbot b√°sico con reglas\n",
    "- Integrar un LLM para respuestas inteligentes\n",
    "- Implementar memoria conversacional\n",
    "- Crear interfaces de usuario con Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Introducci√≥n a los Chatbots\n",
    "\n",
    "### Tipos de chatbots\n",
    "\n",
    "| Tipo | Descripci√≥n | Ejemplo |\n",
    "|------|-------------|--------|\n",
    "| **Basado en reglas** | Respuestas predefinidas, keywords | FAQ bots simples |\n",
    "| **Retrieval-based** | Busca respuestas en base de conocimiento | Customer support |\n",
    "| **Generativo (LLM)** | Genera respuestas din√°micamente | ChatGPT, Claude |\n",
    "| **H√≠brido** | Combina reglas + LLM | Asistentes empresariales |\n",
    "\n",
    "### Arquitectura b√°sica de un chatbot\n",
    "\n",
    "```\n",
    "Usuario ‚Üí [Input] ‚Üí [Procesamiento] ‚Üí [Generaci√≥n] ‚Üí [Output] ‚Üí Usuario\n",
    "                          ‚Üë\n",
    "                     [Memoria/Contexto]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.41 requires requests_mock, which is not installed.\n",
      "tensorflow-intel 2.17.0 requires ml-dtypes<0.5.0,>=0.3.1, but you have ml-dtypes 0.5.4 which is incompatible.\n",
      "tensorflow-intel 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
      "tensorflow-intel 2.17.0 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.20.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires nbformat==5.4.0, but you have nbformat 5.7.0 which is incompatible.\n",
      "conda-repo-cli 1.0.41 requires requests==2.28.1, but you have requests 2.32.5 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\frane\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q langchain langchain-groq langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu GROQ API Key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key configurada ‚úì\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Setup Groq API (FREE tier)\n",
    "if 'GROQ_API_KEY' not in os.environ:\n",
    "    os.environ['GROQ_API_KEY'] = getpass(\"Introduce tu GROQ API Key: \")\n",
    "\n",
    "print(\"API Key configurada ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"reglas\"></a>\n",
    "## 2. Chatbot con reglas simples\n",
    "\n",
    "El chatbot m√°s simple utiliza coincidencia de palabras clave para seleccionar respuestas predefinidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot basado en reglas:\n",
      "==================================================\n",
      "üë§ Usuario: Hola, ¬øqu√© tal?\n",
      "ü§ñ Bot: ¬°Hola! ¬øEn qu√© puedo ayudarte?\n",
      "\n",
      "üë§ Usuario: ¬øCu√°l es el precio del producto?\n",
      "ü§ñ Bot: Nuestros precios var√≠an seg√∫n el producto. ¬øCu√°l te interesa?\n",
      "\n",
      "üë§ Usuario: ¬øCu√°l es su horario?\n",
      "ü§ñ Bot: Nuestro horario es de 9:00 a 18:00, de lunes a viernes.\n",
      "\n",
      "üë§ Usuario: Quiero comprar un coche\n",
      "ü§ñ Bot: Lo siento, no entend√≠ tu pregunta. ¬øPuedes reformularla?\n",
      "\n",
      "üë§ Usuario: Gracias por la ayuda\n",
      "ü§ñ Bot: ¬°De nada! ¬øNecesitas algo m√°s?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RuleBasedChatbot:\n",
    "    \"\"\"Simple rule-based chatbot using keyword matching.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define rules as keyword -> response mappings\n",
    "        self.rules = {\n",
    "            'hola': '¬°Hola! ¬øEn qu√© puedo ayudarte?',\n",
    "            'buenos d√≠as': '¬°Buenos d√≠as! ¬øC√≥mo est√°s?',\n",
    "            'adi√≥s': '¬°Hasta luego! Que tengas un buen d√≠a.',\n",
    "            'gracias': '¬°De nada! ¬øNecesitas algo m√°s?',\n",
    "            'precio': 'Nuestros precios var√≠an seg√∫n el producto. ¬øCu√°l te interesa?',\n",
    "            'horario': 'Nuestro horario es de 9:00 a 18:00, de lunes a viernes.',\n",
    "            'contacto': 'Puedes contactarnos en info@ejemplo.com o al 900 123 456.',\n",
    "            'ayuda': 'Puedo ayudarte con: precios, horarios, contacto. ¬øQu√© necesitas?'\n",
    "        }\n",
    "        self.default_response = \"Lo siento, no entend√≠ tu pregunta. ¬øPuedes reformularla?\"\n",
    "    \n",
    "    def respond(self, user_input: str) -> str:\n",
    "        \"\"\"Generate response based on keyword matching.\"\"\"\n",
    "        user_input_lower = user_input.lower()\n",
    "        \n",
    "        # Check each rule\n",
    "        for keyword, response in self.rules.items():\n",
    "            if keyword in user_input_lower:\n",
    "                return response\n",
    "        \n",
    "        return self.default_response\n",
    "\n",
    "# Test the rule-based chatbot\n",
    "bot = RuleBasedChatbot()\n",
    "\n",
    "test_messages = [\n",
    "    \"Hola, ¬øqu√© tal?\",\n",
    "    \"¬øCu√°l es el precio del producto?\",\n",
    "    \"¬øCu√°l es su horario?\",\n",
    "    \"Quiero comprar un coche\",  # No matching rule\n",
    "    \"Gracias por la ayuda\"\n",
    "]\n",
    "\n",
    "print(\"Chatbot basado en reglas:\")\n",
    "print(\"=\" * 50)\n",
    "for msg in test_messages:\n",
    "    response = bot.respond(msg)\n",
    "    print(f\"üë§ Usuario: {msg}\")\n",
    "    print(f\"ü§ñ Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones del chatbot basado en reglas\n",
    "\n",
    "| ‚úÖ Ventajas | ‚ùå Desventajas |\n",
    "|------------|---------------|\n",
    "| R√°pido y predecible | No entiende contexto |\n",
    "| Sin costos de API | Respuestas limitadas |\n",
    "| F√°cil de mantener | No maneja variaciones |\n",
    "| Control total | No aprende |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"llm\"></a>\n",
    "## 3. Chatbot con LLM (API gratuita)\n",
    "\n",
    "Ahora crearemos un chatbot m√°s inteligente usando un LLM a trav√©s de Groq (tier gratuito)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot con LLM:\n",
      "==================================================\n",
      "üë§ Usuario: Hola, ¬øqu√© tal?\n",
      "ü§ñ Bot: ¬°Hola! Estoy bien, gracias. ¬øEn qu√© puedo ayudarte hoy?\n",
      "\n",
      "üë§ Usuario: ¬øPuedes explicarme qu√© es machine learning en t√©rminos simples?\n",
      "ü§ñ Bot: ¬°Claro! El **machine learning** (aprendizaje autom√°tico) es una t√©cnica que permite a las computadoras \"aprender\" de los datos y mejorar su rendimiento en tareas espec√≠ficas sin ser programadas expl√≠citamente.\n",
      "\n",
      "**Funciona de la siguiente manera:**\n",
      "\n",
      "1. Se proporciona a la computadora un conjunto de datos (como im√°genes, textos o n√∫meros).\n",
      "2. La computadora busca patrones y relaciones en los datos.\n",
      "3. A partir de estos patrones, la computadora crea un modelo que puede predecir o clasificar nuevos datos.\n",
      "\n",
      "**Ejemplos:**\n",
      "\n",
      "* Un sistema de recomendaci√≥n de pel√≠culas que sugiere pel√≠culas basadas en tus gustos previos.\n",
      "* Un software de reconocimiento de voz que aprende a entender tu acento y vocabulario.\n",
      "\n",
      "En resumen, el machine learning es como ense√±ar a una computadora a aprender de los datos y tomar decisiones por s√≠ misma. ¬øQuieres saber m√°s sobre alg√∫n aspecto espec√≠fico del machine learning?\n",
      "\n",
      "üë§ Usuario: Quiero comprar un coche, ¬øqu√© consejos me das?\n",
      "ü§ñ Bot: ¬°Claro! Aqu√≠ te dejo algunos consejos para comprar un coche:\n",
      "\n",
      "1. **Define tu presupuesto**: Establece un l√≠mite de gasto y considera todos los costos, incluyendo el precio del coche, impuestos, seguro, mantenimiento y combustible.\n",
      "2. **Investiga y compara**: Busca diferentes modelos y marcas, y compara sus caracter√≠sticas, precios y rese√±as de otros propietarios.\n",
      "3. **Verifica el historial del coche**: Si est√°s considerando un coche usado, verifica su historial de accidentes, mantenimiento y due√±os anteriores.\n",
      "4. **Prueba el coche**: Conduce el coche antes de comprarlo para asegurarte de que se adapte a tus necesidades y preferencias.\n",
      "5. **Negocia el precio**: No tengas miedo de negociar el precio del coche, especialmente si est√°s comprando un coche usado.\n",
      "6. **Considera la financiaci√≥n**: Si no tienes el dinero para pagar el coche en efectivo, investiga opciones de financiaci√≥n y compara las tasas de inter√©s y los t√©rminos de los pr√©stamos.\n",
      "7. **Lee las condiciones del contrato**: Antes de firmar el contrato, aseg√∫rate de entender todas las condiciones y t√©rminos de la venta.\n",
      "\n",
      "Recuerda que la compra de un coche es una decisi√≥n importante, as√≠ que t√≥mate tu tiempo y no te apresures. ¬°Buena suerte!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "class LLMChatbot:\n",
    "    \"\"\"Chatbot powered by LLM (using free Groq API).\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None):\n",
    "        self.llm = ChatGroq(\n",
    "            model_name=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Default system prompt\n",
    "        self.system_prompt = system_prompt or \"\"\"Eres un asistente amable y servicial.\n",
    "        Responde de forma concisa y clara.\n",
    "        Si no sabes algo, dilo honestamente.\"\"\"\n",
    "    \n",
    "    def respond(self, user_input: str) -> str:\n",
    "        \"\"\"Generate response using LLM.\"\"\"\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=user_input)\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "# Test the LLM chatbot\n",
    "llm_bot = LLMChatbot()\n",
    "\n",
    "test_messages = [\n",
    "    \"Hola, ¬øqu√© tal?\",\n",
    "    \"¬øPuedes explicarme qu√© es machine learning en t√©rminos simples?\",\n",
    "    \"Quiero comprar un coche, ¬øqu√© consejos me das?\"\n",
    "]\n",
    "\n",
    "print(\"Chatbot con LLM:\")\n",
    "print(\"=\" * 50)\n",
    "for msg in test_messages:\n",
    "    print(f\"üë§ Usuario: {msg}\")\n",
    "    response = llm_bot.respond(msg)\n",
    "    print(f\"ü§ñ Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot especializado con rol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot de Soporte T√©cnico:\n",
      "==================================================\n",
      "üë§ Usuario: Mi laptop no enciende, ¬øqu√© puedo hacer?\n",
      "ü§ñ Bot: Lo siento mucho que est√©s experimentando problemas con tu laptop. Antes de nada, te recomiendo verificar algunos puntos b√°sicos para asegurarnos de que no se trata de un problema simple de resoluci√≥n inmediata.\n",
      "\n",
      "1. **Carga la bater√≠a**: Aseg√∫rate de que la bater√≠a de tu laptop est√© completamente cargada o, si es posible, intenta conectarla directamente a la corriente el√©ctrica.\n",
      "2. **Verifica el cable de alimentaci√≥n**: Revisa el cable de alimentaci√≥n para asegurarte de que no est√© da√±ado y que est√© conectado correctamente tanto a la laptop como a la toma de corriente.\n",
      "3. **Bot√≥n de encendido**: Aseg√∫rate de que est√°s presionando el bot√≥n de encendido correcto y de la manera adecuada.\n",
      "\n",
      "Si despu√©s de verificar estos puntos, tu laptop sigue sin encenderse, es posible que se trate de un problema m√°s serio que requiera atenci√≥n t√©cnica especializada.\n",
      "\n",
      "**Opciones para seguir:**\n",
      "\n",
      "- **Garant√≠a**: Si tu laptop todav√≠a est√° dentro del plazo de garant√≠a (2 a√±os desde la fecha de compra), podemos ayudarte a procesar una reparaci√≥n o reemplazo seg√∫n corresponda. Por favor, ten a mano el n√∫mero de serie de tu laptop y la fecha de compra para verificar el estado de tu garant√≠a.\n",
      "- **Soporte t√©cnico**: Puedes contactarnos por tel√©fono al 900 111 222 o enviarnos un correo electr√≥nico a soporte@techstore.com para obtener asesoramiento m√°s detallado. Estamos disponible de lunes a viernes de 9:00 a 18:00.\n",
      "- **Visita a una tienda**: Si el problema requiere una evaluaci√≥n f√≠sica de tu laptop, te recomendamos visitar una de nuestras tiendas. Nuestro equipo estar√° encantado de ayudarte a diagnosticar y resolver el problema de la manera m√°s eficiente posible.\n",
      "\n",
      "Recuerda que estamos aqu√≠ para ayudarte. Por favor, no dudes en hacernos saber si necesitas m√°s orientaci√≥n o asistencia.\n",
      "\n",
      "üë§ Usuario: ¬øCu√°nto dura la garant√≠a de los smartphones?\n",
      "ü§ñ Bot: La garant√≠a de nuestros smartphones es de 1 a√±o. Si tienes alg√∫n problema con tu dispositivo dentro de ese plazo, no dudes en hac√©rmelo saber y estar√© encantado de ayudarte. ¬øNecesitas asistencia con algo m√°s o tienes alguna pregunta adicional sobre nuestros productos?\n",
      "\n",
      "üë§ Usuario: ¬øTienen servicio de reparaci√≥n los fines de semana?\n",
      "ü§ñ Bot: Lo siento, pero nuestro horario de soporte es de lunes a viernes, de 9:00 a 18:00. No ofrecemos servicio de reparaci√≥n los fines de semana. Si necesita asistencia con alguno de nuestros productos, puede contactarnos a trav√©s del correo electr√≥nico [soporte@techstore.com](mailto:soporte@techstore.com) o llamarnos al 900 111 222 dentro de nuestro horario de atenci√≥n.\n",
      "\n",
      "Si el problema requiere atenci√≥n presencial, le recomendamos visitar una de nuestras tiendas durante el horario de apertura. All√≠, nuestros t√©cnicos podr√°n evaluar y reparar su dispositivo de manera eficiente. Estamos aqu√≠ para ayudarle de lunes a viernes, as√≠ que no dude en contactarnos durante nuestro horario de atenci√≥n. ¬øHay algo m√°s en lo que pueda ayudarle?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a specialized chatbot for tech support\n",
    "tech_support_prompt = \"\"\"Eres un t√©cnico de soporte experto en productos tecnol√≥gicos.\n",
    "Tu empresa vende:\n",
    "- Laptops (garant√≠a 2 a√±os)\n",
    "- Smartphones (garant√≠a 1 a√±o)  \n",
    "- Tablets (garant√≠a 1 a√±o)\n",
    "\n",
    "Horario de soporte: Lunes a Viernes, 9:00-18:00\n",
    "Email: soporte@techstore.com\n",
    "Tel√©fono: 900 111 222\n",
    "\n",
    "Responde de forma profesional pero amigable.\n",
    "Si el problema requiere atenci√≥n presencial, recomienda visitar una tienda.\"\"\"\n",
    "\n",
    "tech_bot = LLMChatbot(system_prompt=tech_support_prompt)\n",
    "\n",
    "tech_questions = [\n",
    "    \"Mi laptop no enciende, ¬øqu√© puedo hacer?\",\n",
    "    \"¬øCu√°nto dura la garant√≠a de los smartphones?\",\n",
    "    \"¬øTienen servicio de reparaci√≥n los fines de semana?\"\n",
    "]\n",
    "\n",
    "print(\"Chatbot de Soporte T√©cnico:\")\n",
    "print(\"=\" * 50)\n",
    "for q in tech_questions:\n",
    "    print(f\"üë§ Usuario: {q}\")\n",
    "    response = tech_bot.respond(q)\n",
    "    print(f\"ü§ñ Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"memoria\"></a>\n",
    "## 4. A√±adiendo memoria a la conversaci√≥n\n",
    "\n",
    "Un chatbot sin memoria no puede mantener una conversaci√≥n coherente. Vamos a implementar memoria conversacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot con memoria:\n",
      "==================================================\n",
      "üë§ Usuario: Hola, me llamo Carlos.\n",
      "ü§ñ Bot: **Hola Carlos**\n",
      "\n",
      "Me alegra conocerte. Soy un asistente amable y servicial, aqu√≠ para ayudarte con cualquier pregunta o tema que desees discutir. ¬øEn qu√© puedo ayudarte hoy? ¬øTienes alguna pregunta o necesitas ayuda con algo en particular? Estoy aqu√≠ para escucharte y proporcionarte la mejor ayuda posible.\n",
      "\n",
      "üë§ Usuario: ¬øCu√°l es la capital de Francia?\n",
      "ü§ñ Bot: **La capital de Francia es Par√≠s**\n",
      "\n",
      "Par√≠s es una de las ciudades m√°s emblem√°ticas y famosas del mundo, conocida por sus monumentos hist√≥ricos como la Torre Eiffel, el Louvre y Notre Dame. Es un destino tur√≠stico muy popular y un centro cultural y econ√≥mico importante en Europa.\n",
      "\n",
      "¬øQuieres saber m√°s sobre Par√≠s o Francia? Estoy aqu√≠ para ayudarte con cualquier pregunta que tengas.\n",
      "\n",
      "üë§ Usuario: ¬øY cu√°ntos habitantes tiene?\n",
      "ü§ñ Bot: **Poblaci√≥n de Par√≠s**\n",
      "\n",
      "Seg√∫n los datos m√°s recientes, la ciudad de Par√≠s tiene una poblaci√≥n de aproximadamente **2,1 millones de habitantes**. Sin embargo, si consideramos la regi√≥n metropolitana de Par√≠s, que incluye los suburbios y √°reas circundantes, la poblaci√≥n se eleva a unos **12,2 millones de habitantes**, lo que la convierte en una de las √°reas metropolitanas m√°s grandes de Europa.\n",
      "\n",
      "¬øQuieres saber m√°s sobre la demograf√≠a de Par√≠s o Francia? Estoy aqu√≠ para proporcionarte m√°s informaci√≥n.\n",
      "\n",
      "üë§ Usuario: ¬øC√≥mo me llamo?\n",
      "ü§ñ Bot: **Tu nombre es Carlos**\n",
      "\n",
      "Me lo dijiste al principio de nuestra conversaci√≥n. ¬øRecuerdas? Me presentaste con un \"Hola, me llamo Carlos\". Estoy aqu√≠ para ayudarte y recordar los detalles de nuestra conversaci√≥n. ¬øEn qu√© m√°s puedo ayudarte?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ChatbotWithMemory:\n",
    "    \"\"\"Chatbot with conversation memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None):\n",
    "        self.llm = ChatGroq(\n",
    "            model_name=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = system_prompt or \"Eres un asistente amable y servicial.\"\n",
    "        \n",
    "        # Initialize conversation history\n",
    "        self.history = []\n",
    "    \n",
    "    def respond(self, user_input: str) -> str:\n",
    "        \"\"\"Generate response while maintaining conversation history.\"\"\"\n",
    "        # Build messages with history\n",
    "        messages = [SystemMessage(content=self.system_prompt)]\n",
    "        \n",
    "        # Add conversation history\n",
    "        for human_msg, ai_msg in self.history:\n",
    "            messages.append(HumanMessage(content=human_msg))\n",
    "            messages.append(AIMessage(content=ai_msg))\n",
    "        \n",
    "        # Add current message\n",
    "        messages.append(HumanMessage(content=user_input))\n",
    "        \n",
    "        # Get response\n",
    "        response = self.llm.invoke(messages)\n",
    "        ai_response = response.content\n",
    "        \n",
    "        # Save to history\n",
    "        self.history.append((user_input, ai_response))\n",
    "        \n",
    "        return ai_response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.history = []\n",
    "        print(\"Historial borrado.\")\n",
    "\n",
    "# Test chatbot with memory\n",
    "memory_bot = ChatbotWithMemory()\n",
    "\n",
    "print(\"Chatbot con memoria:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Conversation that requires memory\n",
    "conversation = [\n",
    "    \"Hola, me llamo Carlos.\",\n",
    "    \"¬øCu√°l es la capital de Francia?\",\n",
    "    \"¬øY cu√°ntos habitantes tiene?\",\n",
    "    \"¬øC√≥mo me llamo?\"  # Test if bot remembers the name\n",
    "]\n",
    "\n",
    "for msg in conversation:\n",
    "    print(f\"üë§ Usuario: {msg}\")\n",
    "    response = memory_bot.respond(msg)\n",
    "    print(f\"ü§ñ Bot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historial de la conversaci√≥n:\n",
      "==================================================\n",
      "Turno 1:\n",
      "  üë§ Human: Hola, me llamo Carlos.\n",
      "  ü§ñ AI: **Hola Carlos**\n",
      "\n",
      "Me alegra conocerte. Soy un asist...\n",
      "\n",
      "Turno 2:\n",
      "  üë§ Human: ¬øCu√°l es la capital de Francia?\n",
      "  ü§ñ AI: **La capital de Francia es Par√≠s**\n",
      "\n",
      "Par√≠s es una d...\n",
      "\n",
      "Turno 3:\n",
      "  üë§ Human: ¬øY cu√°ntos habitantes tiene?\n",
      "  ü§ñ AI: **Poblaci√≥n de Par√≠s**\n",
      "\n",
      "Seg√∫n los datos m√°s recien...\n",
      "\n",
      "Turno 4:\n",
      "  üë§ Human: ¬øC√≥mo me llamo?\n",
      "  ü§ñ AI: **Tu nombre es Carlos**\n",
      "\n",
      "Me lo dijiste al principi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show conversation history\n",
    "print(\"Historial de la conversaci√≥n:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (human, ai) in enumerate(memory_bot.history, 1):\n",
    "    print(f\"Turno {i}:\")\n",
    "    print(f\"  üë§ Human: {human[:50]}...\" if len(human) > 50 else f\"  üë§ Human: {human}\")\n",
    "    print(f\"  ü§ñ AI: {ai[:50]}...\" if len(ai) > 50 else f\"  ü§ñ AI: {ai}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gesti√≥n de memoria: Sliding Window\n",
    "\n",
    "Para conversaciones largas, mantener todo el historial es costoso. Una soluci√≥n es usar una ventana deslizante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot con memoria de 3 turnos\n"
     ]
    }
   ],
   "source": [
    "class ChatbotWithSlidingWindow:\n",
    "    \"\"\"Chatbot with sliding window memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt: str = None, max_history: int = 5):\n",
    "        self.llm = ChatGroq(\n",
    "            model_name=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = system_prompt or \"Eres un asistente amable.\"\n",
    "        self.history = []\n",
    "        self.max_history = max_history  # Maximum number of turns to remember\n",
    "    \n",
    "    def respond(self, user_input: str) -> str:\n",
    "        \"\"\"Generate response with sliding window memory.\"\"\"\n",
    "        messages = [SystemMessage(content=self.system_prompt)]\n",
    "        \n",
    "        # Only include last N turns\n",
    "        recent_history = self.history[-self.max_history:]\n",
    "        \n",
    "        for human_msg, ai_msg in recent_history:\n",
    "            messages.append(HumanMessage(content=human_msg))\n",
    "            messages.append(AIMessage(content=ai_msg))\n",
    "        \n",
    "        messages.append(HumanMessage(content=user_input))\n",
    "        \n",
    "        response = self.llm.invoke(messages)\n",
    "        ai_response = response.content\n",
    "        \n",
    "        self.history.append((user_input, ai_response))\n",
    "        \n",
    "        return ai_response\n",
    "\n",
    "# Test sliding window\n",
    "sliding_bot = ChatbotWithSlidingWindow(max_history=3)\n",
    "print(f\"Bot con memoria de {sliding_bot.max_history} turnos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"streamlit\"></a>\n",
    "## 5. Interfaces con Streamlit\n",
    "\n",
    "**Streamlit** permite crear interfaces web interactivas f√°cilmente. A continuaci√≥n se muestra el c√≥digo para una aplicaci√≥n de chatbot.\n",
    "\n",
    "**Nota:** Este c√≥digo debe ejecutarse como script de Python, no en Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for a Streamlit chatbot app\n",
    "# Save as 'chatbot_app.py' and run with: streamlit run chatbot_app.py\n",
    "\n",
    "streamlit_code = '''\n",
    "import streamlit as st\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "import os\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "    page_title=\"Chatbot con LLM\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"centered\"\n",
    ")\n",
    "\n",
    "st.title(\"ü§ñ Chatbot con LLM\")\n",
    "\n",
    "# Sidebar for API key\n",
    "with st.sidebar:\n",
    "    st.header(\"Configuraci√≥n\")\n",
    "    api_key = st.text_input(\"GROQ API Key\", type=\"password\")\n",
    "    \n",
    "    if api_key:\n",
    "        os.environ[\"GROQ_API_KEY\"] = api_key\n",
    "    \n",
    "    if st.button(\"Limpiar historial\"):\n",
    "        st.session_state.messages = []\n",
    "        st.rerun()\n",
    "\n",
    "# Initialize session state for chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "if prompt := st.chat_input(\"Escribe tu mensaje...\"):\n",
    "    if not api_key:\n",
    "        st.error(\"Por favor, introduce tu API key en la barra lateral.\")\n",
    "    else:\n",
    "        # Add user message to history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(prompt)\n",
    "        \n",
    "        # Generate response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            with st.spinner(\"Pensando...\"):\n",
    "                try:\n",
    "                    llm = ChatGroq(\n",
    "                        model_name=\"llama-3.3-70b-versatile\",\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "                    \n",
    "                    # Build messages\n",
    "                    messages = [SystemMessage(content=\"Eres un asistente amable.\")]\n",
    "                    \n",
    "                    for msg in st.session_state.messages:\n",
    "                        if msg[\"role\"] == \"user\":\n",
    "                            messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "                        else:\n",
    "                            messages.append(AIMessage(content=msg[\"content\"]))\n",
    "                    \n",
    "                    response = llm.invoke(messages)\n",
    "                    st.write(response.content)\n",
    "                    \n",
    "                    # Add assistant response to history\n",
    "                    st.session_state.messages.append({\n",
    "                        \"role\": \"assistant\", \n",
    "                        \"content\": response.content\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error: {e}\")\n",
    "'''\n",
    "\n",
    "print(\"C√≥digo para Streamlit Chatbot:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Guarda este c√≥digo como 'chatbot_app.py'\")\n",
    "print(\"Ejecuta con: streamlit run chatbot_app.py\")\n",
    "print(\"=\"*50)\n",
    "print(streamlit_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Streamlit app to a file\n",
    "with open('chatbot_app.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"‚úì Archivo 'chatbot_app.py' creado.\")\n",
    "print(\"Para ejecutar: streamlit run chatbot_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 6. Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Chatbot FAQ personalizado\n",
    "\n",
    "Mejora el chatbot basado en reglas para manejar m√°s casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a more sophisticated rule-based chatbot\n",
    "# Add rules for:\n",
    "# - Product returns\n",
    "# - Payment methods\n",
    "# - Shipping information\n",
    "# - Opening hours\n",
    "\n",
    "class EnhancedRuleChatbot:\n",
    "    def __init__(self):\n",
    "        self.rules = {\n",
    "            # Add your rules here\n",
    "            'hola': '¬°Hola! ¬øEn qu√© puedo ayudarte?',\n",
    "            # 'devolucion': '...',\n",
    "            # 'pago': '...',\n",
    "            # etc.\n",
    "        }\n",
    "        self.default_response = \"No entend√≠. ¬øPuedes reformular?\"\n",
    "    \n",
    "    def respond(self, user_input: str) -> str:\n",
    "        user_input_lower = user_input.lower()\n",
    "        for keyword, response in self.rules.items():\n",
    "            if keyword in user_input_lower:\n",
    "                return response\n",
    "        return self.default_response\n",
    "\n",
    "# Test your chatbot\n",
    "# enhanced_bot = EnhancedRuleChatbot()\n",
    "# print(enhanced_bot.respond(\"¬øC√≥mo puedo devolver un producto?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Chatbot con personalidad\n",
    "\n",
    "Crea un chatbot con una personalidad espec√≠fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a chatbot with a specific personality\n",
    "# Ideas:\n",
    "# - A pirate who speaks in pirate language\n",
    "# - A Yoda-like character\n",
    "# - A formal butler\n",
    "# - A cheerful fitness coach\n",
    "\n",
    "personality_prompt = \"\"\"\n",
    "# Define your personality prompt here\n",
    "# Include:\n",
    "# - Character description\n",
    "# - Speaking style\n",
    "# - Typical phrases\n",
    "# - Topics they're expert in\n",
    "\"\"\"\n",
    "\n",
    "# personality_bot = LLMChatbot(system_prompt=personality_prompt)\n",
    "# Test with some messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Chatbot h√≠brido\n",
    "\n",
    "Combina reglas con LLM: usa reglas para casos conocidos y LLM para el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Hybrid chatbot (rules + LLM)\n",
    "\n",
    "class HybridChatbot:\n",
    "    \"\"\"Chatbot that uses rules for known cases and LLM for others.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define rules for FAQ\n",
    "        self.rules = {\n",
    "            'horario': 'Horario: L-V 9:00-18:00',\n",
    "            'precio': 'Consulta precios en www.ejemplo.com/precios',\n",
    "            # Add more rules...\n",
    "        }\n",
    "        \n",
    "        # LLM for complex questions\n",
    "        self.llm = ChatGroq(\n",
    "            model_name=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    def respond(self, user_input: str) -> str:\n",
    "        # First, try rules\n",
    "        user_input_lower = user_input.lower()\n",
    "        for keyword, response in self.rules.items():\n",
    "            if keyword in user_input_lower:\n",
    "                return f\"[REGLA] {response}\"\n",
    "        \n",
    "        # If no rule matches, use LLM\n",
    "        messages = [\n",
    "            SystemMessage(content=\"Eres un asistente de tienda online.\"),\n",
    "            HumanMessage(content=user_input)\n",
    "        ]\n",
    "        response = self.llm.invoke(messages)\n",
    "        return f\"[LLM] {response.content}\"\n",
    "\n",
    "# Test hybrid chatbot\n",
    "# hybrid = HybridChatbot()\n",
    "# print(hybrid.respond(\"¬øCu√°l es el horario?\"))  # Should use rule\n",
    "# print(hybrid.respond(\"¬øQu√© opinas del cambio clim√°tico?\"))  # Should use LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **Chatbots basados en reglas**: Simples pero limitados\n",
    "2. **Chatbots con LLM**: Inteligentes y flexibles\n",
    "3. **Memoria conversacional**: Fundamental para coherencia\n",
    "4. **Sliding window**: Gesti√≥n eficiente de memoria\n",
    "5. **Streamlit**: Interfaces web interactivas\n",
    "\n",
    "### Tipos de memoria en chatbots\n",
    "\n",
    "| Tipo | Descripci√≥n | Uso |\n",
    "|------|-------------|-----|\n",
    "| Sin memoria | Cada mensaje es independiente | Consultas simples |\n",
    "| Memoria completa | Guarda todo el historial | Conversaciones cortas |\n",
    "| Sliding window | Guarda √∫ltimos N turnos | Conversaciones largas |\n",
    "| Resumen | Guarda resumen de la conversaci√≥n | Conversaciones muy largas |\n",
    "\n",
    "En el siguiente notebook veremos **Vector Stores y Retrieval**, fundamentales para crear chatbots con acceso a informaci√≥n externa.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [LangChain Chat Models](https://python.langchain.com/docs/modules/model_io/chat/)\n",
    "- [Streamlit Documentation](https://docs.streamlit.io/)\n",
    "- [Groq Console](https://console.groq.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
