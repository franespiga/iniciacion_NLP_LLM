{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - IntroducciÃ³n a RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "## Curso de LLMs y Aplicaciones de IA\n",
    "\n",
    "**DuraciÃ³n estimada:** 2.5-3 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Ãndice\n",
    "\n",
    "1. [Â¿QuÃ© es RAG?](#intro)\n",
    "2. [Arquitectura de un sistema RAG](#arquitectura)\n",
    "3. [ImplementaciÃ³n paso a paso](#implementacion)\n",
    "4. [RAG con LangChain](#langchain)\n",
    "5. [Tipos de memoria en RAG](#memoria)\n",
    "6. [Mejores prÃ¡cticas](#mejores)\n",
    "7. [Ejercicios prÃ¡cticos](#ejercicios)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, serÃ¡s capaz de:\n",
    "- Entender quÃ© es RAG y cuÃ¡ndo usarlo\n",
    "- Implementar un sistema RAG bÃ¡sico\n",
    "- Integrar retrieval con generaciÃ³n de respuestas\n",
    "- AÃ±adir memoria conversacional al RAG\n",
    "- Aplicar mejores prÃ¡cticas de diseÃ±o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## 1. Â¿QuÃ© es RAG?\n",
    "\n",
    "**RAG** (Retrieval-Augmented Generation) es una tÃ©cnica que combina:\n",
    "1. **Retrieval**: Buscar informaciÃ³n relevante en una base de conocimiento\n",
    "2. **Augmented**: AÃ±adir esa informaciÃ³n al contexto del LLM\n",
    "3. **Generation**: Generar respuestas basadas en el contexto aumentado\n",
    "\n",
    "### Â¿Por quÃ© RAG?\n",
    "\n",
    "| Problema del LLM | SoluciÃ³n con RAG |\n",
    "|-----------------|------------------|\n",
    "| Conocimiento desactualizado | Acceso a informaciÃ³n actual |\n",
    "| No conoce datos privados | Acceso a documentos corporativos |\n",
    "| Alucinaciones | Respuestas basadas en fuentes |\n",
    "| Sin citaciÃ³n | Puede citar fuentes |\n",
    "\n",
    "### RAG vs Fine-tuning\n",
    "\n",
    "| Aspecto | RAG | Fine-tuning |\n",
    "|---------|-----|-------------|\n",
    "| Costo | Bajo | Alto |\n",
    "| ActualizaciÃ³n | FÃ¡cil | Requiere re-entrenar |\n",
    "| Explicabilidad | Alta (cita fuentes) | Baja |\n",
    "| Conocimiento privado | âœ… | âœ… |\n",
    "| Latencia | Mayor | Menor |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (all free)\n",
    "#!pip install -q langchain langchain-community langchain-groq langchain-huggingface\n",
    "#!pip install -q faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu GROQ API Key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfiguraciÃ³n completada âœ“\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup Groq API (FREE tier)\n",
    "if 'GROQ_API_KEY' not in os.environ:\n",
    "    os.environ['GROQ_API_KEY'] = getpass(\"Introduce tu GROQ API Key: \")\n",
    "\n",
    "print(\"ConfiguraciÃ³n completada âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"arquitectura\"></a>\n",
    "## 2. Arquitectura de un sistema RAG\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      FASE DE INDEXACIÃ“N                         â”‚\n",
    "â”‚  Documentos â†’ [Loader] â†’ [Splitter] â†’ [Embeddings] â†’ VectorDB  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      FASE DE CONSULTA                           â”‚\n",
    "â”‚  Query â†’ [Embedding] â†’ [Retrieval] â†’ Contexto                  â”‚\n",
    "â”‚                                          â†“                      â”‚\n",
    "â”‚  Query + Contexto â†’ [LLM] â†’ Respuesta                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"implementacion\"></a>\n",
    "## 3. ImplementaciÃ³n paso a paso\n",
    "\n",
    "Vamos a construir un RAG para responder preguntas sobre una base de conocimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de conocimiento creada: 6 documentos\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create knowledge base\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample knowledge base about a fictional company\n",
    "knowledge_base = [\n",
    "    Document(\n",
    "        page_content=\"TechCorp fue fundada en 2015 en Madrid por MarÃ­a GarcÃ­a y Juan LÃ³pez. \"\n",
    "                     \"La empresa se especializa en soluciones de inteligencia artificial para empresas.\",\n",
    "        metadata={\"source\": \"about.txt\", \"section\": \"historia\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Los productos principales de TechCorp incluyen: AIAssist (asistente virtual), \"\n",
    "                     \"DataVision (anÃ¡lisis de datos), y CloudBrain (procesamiento en la nube).\",\n",
    "        metadata={\"source\": \"products.txt\", \"section\": \"productos\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"TechCorp tiene oficinas en Madrid (sede central), Barcelona y Valencia. \"\n",
    "                     \"Actualmente cuenta con 150 empleados.\",\n",
    "        metadata={\"source\": \"about.txt\", \"section\": \"oficinas\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"El horario de atenciÃ³n al cliente es de lunes a viernes, de 9:00 a 18:00. \"\n",
    "                     \"Email de soporte: soporte@techcorp.es. TelÃ©fono: 900 123 456.\",\n",
    "        metadata={\"source\": \"contact.txt\", \"section\": \"contacto\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"TechCorp ofrece planes de precios: BÃ¡sico (99â‚¬/mes), Profesional (299â‚¬/mes), \"\n",
    "                     \"y Enterprise (personalizado). Todos incluyen soporte tÃ©cnico.\",\n",
    "        metadata={\"source\": \"pricing.txt\", \"section\": \"precios\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"La polÃ­tica de devoluciones permite cancelar en los primeros 30 dÃ­as \"\n",
    "                     \"con reembolso completo. DespuÃ©s, se aplica un cargo del 20%.\",\n",
    "        metadata={\"source\": \"policies.txt\", \"section\": \"devoluciones\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Base de conocimiento creada: {len(knowledge_base)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo de embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 1070.65it/s, Materializing param=pooler.dense.weight]\n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando vector store...\n",
      "Vector store creado con 6 vectores\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create embeddings and vector store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load free embedding model\n",
    "print(\"Cargando modelo de embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "print(\"Creando vector store...\")\n",
    "vector_store = FAISS.from_documents(knowledge_base, embeddings)\n",
    "\n",
    "print(f\"Vector store creado con {vector_store.index.ntotal} vectores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Â¿CuÃ¡les son los precios?'\n",
      "\n",
      "Documentos recuperados: 3\n",
      "\n",
      "1. TechCorp tiene oficinas en Madrid (sede central), Barcelona y Valencia. Actualme...\n",
      "\n",
      "2. La polÃ­tica de devoluciones permite cancelar en los primeros 30 dÃ­as con reembol...\n",
      "\n",
      "3. TechCorp ofrece planes de precios: BÃ¡sico (99â‚¬/mes), Profesional (299â‚¬/mes), y E...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 documents\n",
    ")\n",
    "\n",
    "# Test retriever\n",
    "test_query = \"Â¿CuÃ¡les son los precios?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"\\nDocumentos recuperados: {len(retrieved_docs)}\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM configurado âœ“\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Setup LLM\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"LLM configurado âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt RAG configurado âœ“\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create RAG prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente de atenciÃ³n al cliente de TechCorp.\n",
    "    \n",
    "Responde las preguntas ÃšNICAMENTE basÃ¡ndote en el contexto proporcionado.\n",
    "Si la informaciÃ³n no estÃ¡ en el contexto, di que no tienes esa informaciÃ³n.\n",
    "SÃ© conciso y profesional.\n",
    "\n",
    "Contexto:\n",
    "{context}\"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "print(\"Prompt RAG configurado âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: Â¿CuÃ¡ndo se fundÃ³ la empresa y quiÃ©nes son los fundadores?\n",
      "\n",
      "Respuesta: La empresa TechCorp se fundÃ³ en 2015 en Madrid. Los fundadores son MarÃ­a GarcÃ­a y Juan LÃ³pez.\n",
      "\n",
      "Fuentes: ['about.txt', 'about.txt', 'pricing.txt']\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create RAG chain (manual implementation)\n",
    "def rag_answer(question: str) -> dict:\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    # 1. Retrieve relevant documents\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # 2. Format context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # 3. Create prompt with context\n",
    "    messages = rag_prompt.format_messages(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    # 4. Get LLM response\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": response.content,\n",
    "        \"sources\": [doc.metadata.get(\"source\", \"unknown\") for doc in docs]\n",
    "    }\n",
    "\n",
    "# Test the RAG system\n",
    "result = rag_answer(\"Â¿CuÃ¡ndo se fundÃ³ la empresa y quiÃ©nes son los fundadores?\")\n",
    "\n",
    "print(f\"Pregunta: {result['question']}\")\n",
    "print(f\"\\nRespuesta: {result['answer']}\")\n",
    "print(f\"\\nFuentes: {result['sources']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruebas del sistema RAG\n",
      "============================================================\n",
      "\n",
      "â“ Â¿CuÃ¡les son los productos de TechCorp?\n",
      "ğŸ’¬ Los productos principales de TechCorp son:\n",
      "\n",
      "1. AIAssist (asistente virtual)\n",
      "2. DataVision (anÃ¡lisis de datos)\n",
      "3. CloudBrain (procesamiento en la nube)\n",
      "----------------------------------------\n",
      "\n",
      "â“ Â¿CÃ³mo puedo contactar con soporte?\n",
      "ğŸ’¬ Puedes contactar con nuestro soporte a travÃ©s del telÃ©fono 900 123 456 o enviando un correo electrÃ³nico a soporte@techcorp.es. Nuestro horario de atenciÃ³n es de lunes a viernes, de 9:00 a 18:00.\n",
      "----------------------------------------\n",
      "\n",
      "â“ Â¿CuÃ¡l es la polÃ­tica de devoluciones?\n",
      "ğŸ’¬ La polÃ­tica de devoluciones de TechCorp permite cancelar en los primeros 30 dÃ­as con reembolso completo. DespuÃ©s de este plazo, se aplica un cargo del 20%.\n",
      "----------------------------------------\n",
      "\n",
      "â“ Â¿CuÃ¡ntos empleados tiene la empresa?\n",
      "ğŸ’¬ La empresa TechCorp cuenta con 150 empleados.\n",
      "----------------------------------------\n",
      "\n",
      "â“ Â¿Ofrecen servicios de marketing?\n",
      "ğŸ’¬ No tengo esa informaciÃ³n. Solo sÃ© que se especializan en soluciones de inteligencia artificial para empresas.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "questions = [\n",
    "    \"Â¿CuÃ¡les son los productos de TechCorp?\",\n",
    "    \"Â¿CÃ³mo puedo contactar con soporte?\",\n",
    "    \"Â¿CuÃ¡l es la polÃ­tica de devoluciones?\",\n",
    "    \"Â¿CuÃ¡ntos empleados tiene la empresa?\",\n",
    "    \"Â¿Ofrecen servicios de marketing?\"  # Not in knowledge base\n",
    "]\n",
    "\n",
    "print(\"Pruebas del sistema RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    result = rag_answer(q)\n",
    "    print(f\"\\nâ“ {result['question']}\")\n",
    "    print(f\"ğŸ’¬ {result['answer']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"langchain\"></a>\n",
    "## 4. RAG con LangChain (Simplificado)\n",
    "\n",
    "LangChain proporciona componentes que simplifican la creaciÃ³n de chains RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain creada con LangChain âœ“\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create the document chain\n",
    "system_prompt = \"\"\"Eres un asistente de TechCorp. Responde basÃ¡ndote en el contexto.\n",
    "Si no tienes la informaciÃ³n, dilo claramente.\n",
    "\n",
    "Contexto:\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chains\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "print(\"RAG Chain creada con LangChain âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: Â¿DÃ³nde estÃ¡n las oficinas de TechCorp?\n",
      "\n",
      "Respuesta: Las oficinas de TechCorp estÃ¡n ubicadas en tres ciudades de EspaÃ±a: \n",
      "\n",
      "1. Madrid (sede central)\n",
      "2. Barcelona\n",
      "3. Valencia\n",
      "\n",
      "Contexto usado (3 documentos):\n",
      "  - TechCorp fue fundada en 2015 en Madrid por MarÃ­a GarcÃ­a y Ju...\n",
      "  - TechCorp tiene oficinas en Madrid (sede central), Barcelona ...\n",
      "  - TechCorp ofrece planes de precios: BÃ¡sico (99â‚¬/mes), Profesi...\n"
     ]
    }
   ],
   "source": [
    "# Test the chain\n",
    "response = rag_chain.invoke({\"input\": \"Â¿DÃ³nde estÃ¡n las oficinas de TechCorp?\"})\n",
    "\n",
    "print(f\"Pregunta: {response['input']}\")\n",
    "print(f\"\\nRespuesta: {response['answer']}\")\n",
    "print(f\"\\nContexto usado ({len(response['context'])} documentos):\")\n",
    "for doc in response['context']:\n",
    "    print(f\"  - {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"memoria\"></a>\n",
    "## 5. RAG con Memoria Conversacional\n",
    "\n",
    "AÃ±adir memoria permite que el RAG recuerde el contexto de la conversaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG conversacional configurado âœ“\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Create a store for session histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Update prompt to include chat history\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente de TechCorp. Responde basÃ¡ndote en el contexto.\n",
    "    \n",
    "Contexto de la base de conocimiento:\n",
    "{context}\"\"\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create chain with history\n",
    "qa_chain = create_stuff_documents_chain(llm, contextualize_prompt)\n",
    "rag_chain_with_history = create_retrieval_chain(retriever, qa_chain)\n",
    "\n",
    "# Wrap with message history\n",
    "conversational_rag = RunnableWithMessageHistory(\n",
    "    rag_chain_with_history,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n",
    "\n",
    "print(\"RAG conversacional configurado âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Â¿CuÃ¡les son los planes de precios?\n",
      "A1: Nuestros planes de precios son los siguientes:\n",
      "\n",
      "1. **BÃ¡sico**: 99â‚¬/mes\n",
      "2. **Profesional**: 299â‚¬/mes\n",
      "3. **Enterprise**: Personalizado (el precio se adapta a las necesidades especÃ­ficas de cada cliente)\n",
      "\n",
      "Todos nuestros planes incluyen soporte tÃ©cnico. Â¿Necesitas mÃ¡s informaciÃ³n sobre alguno de ellos?\n",
      "\n",
      "Q2: Â¿Y el mÃ¡s barato incluye soporte?\n",
      "A2: SÃ­, el plan **BÃ¡sico** incluye soporte tÃ©cnico limitado. Esto significa que podrÃ¡s acceder a nuestro centro de ayuda en lÃ­nea y recibirÃ¡s respuesta a tus consultas tÃ©cnicas dentro de un plazo de 24 horas hÃ¡biles.\n",
      "\n",
      "Si necesitas un soporte tÃ©cnico mÃ¡s avanzado o prioritario, te recomiendo considerar el plan **Profesional**, que incluye soporte tÃ©cnico prioritario y acceso a un equipo de expertos que pueden ayudarte a resolver tus dudas de manera mÃ¡s rÃ¡pida y personalizada.\n"
     ]
    }
   ],
   "source": [
    "# Test conversational RAG\n",
    "session_config = {\"configurable\": {\"session_id\": \"user123\"}}\n",
    "\n",
    "# First question\n",
    "response1 = conversational_rag.invoke(\n",
    "    {\"input\": \"Â¿CuÃ¡les son los planes de precios?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q1: Â¿CuÃ¡les son los planes de precios?\")\n",
    "print(f\"A1: {response1['answer']}\\n\")\n",
    "\n",
    "# Follow-up question (should use context from previous)\n",
    "response2 = conversational_rag.invoke(\n",
    "    {\"input\": \"Â¿Y el mÃ¡s barato incluye soporte?\"},\n",
    "    config=session_config\n",
    ")\n",
    "print(f\"Q2: Â¿Y el mÃ¡s barato incluye soporte?\")\n",
    "print(f\"A2: {response2['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"mejores\"></a>\n",
    "## 6. Mejores PrÃ¡cticas\n",
    "\n",
    "### OptimizaciÃ³n de Retrieval\n",
    "\n",
    "| TÃ©cnica | DescripciÃ³n | CuÃ¡ndo usar |\n",
    "|---------|-------------|-------------|\n",
    "| **Aumentar k** | Recuperar mÃ¡s documentos | Respuestas incompletas |\n",
    "| **Chunk overlap** | Solapar fragmentos | Contexto cortado |\n",
    "| **Reranking** | Reordenar resultados | PrecisiÃ³n crÃ­tica |\n",
    "| **Hybrid search** | Combinar keyword + semÃ¡ntico | TÃ©rminos tÃ©cnicos |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegÃºn la informaciÃ³n disponible [1] y [2], TechCorp se fundÃ³ en 2015 en Madrid por MarÃ­a GarcÃ­a y Juan LÃ³pez. Actualmente, la empresa tiene oficinas en Madrid (sede central), Barcelona y Valencia.\n"
     ]
    }
   ],
   "source": [
    "# Example: RAG with source citation\n",
    "def rag_with_sources(question: str) -> str:\n",
    "    \"\"\"Answer with source citations.\"\"\"\n",
    "    # Retrieve\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # Format context with numbered sources\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        context_parts.append(f\"[{i}] ({source}): {doc.page_content}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Responde la pregunta basÃ¡ndote en el contexto.\n",
    "Incluye las referencias [1], [2], etc. cuando cites informaciÃ³n.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Test\n",
    "answer = rag_with_sources(\"Â¿CuÃ¡ndo se fundÃ³ TechCorp y dÃ³nde tiene oficinas?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ejercicios\"></a>\n",
    "## 7. Ejercicios PrÃ¡cticos\n",
    "\n",
    "### Ejercicio 1: RAG para documentaciÃ³n tÃ©cnica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a RAG for API documentation\n",
    "\n",
    "api_docs = [\n",
    "    Document(page_content=\"GET /users - Obtiene lista de usuarios. ParÃ¡metros: limit (int), offset (int).\",\n",
    "             metadata={\"endpoint\": \"/users\", \"method\": \"GET\"}),\n",
    "    Document(page_content=\"POST /users - Crea un nuevo usuario. Body: {name: string, email: string}.\",\n",
    "             metadata={\"endpoint\": \"/users\", \"method\": \"POST\"}),\n",
    "    Document(page_content=\"GET /users/{id} - Obtiene un usuario por ID. Retorna 404 si no existe.\",\n",
    "             metadata={\"endpoint\": \"/users/{id}\", \"method\": \"GET\"}),\n",
    "    Document(page_content=\"AutenticaciÃ³n: Todas las peticiones requieren header 'Authorization: Bearer <token>'.\",\n",
    "             metadata={\"section\": \"auth\"}),\n",
    "    Document(page_content=\"Errores comunes: 401 (no autorizado), 404 (no encontrado), 500 (error servidor).\",\n",
    "             metadata={\"section\": \"errors\"}),\n",
    "]\n",
    "\n",
    "# Create your RAG system for API docs\n",
    "# api_vs = FAISS.from_documents(api_docs, embeddings)\n",
    "# api_retriever = api_vs.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Test questions:\n",
    "# - \"Â¿CÃ³mo creo un usuario nuevo?\"\n",
    "# - \"Â¿QuÃ© significa el error 401?\"\n",
    "# - \"Â¿CÃ³mo me autentico?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: RAG con filtrado por metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: RAG with metadata filtering\n",
    "# Create a system that can filter by document section\n",
    "\n",
    "def rag_with_filter(question: str, section_filter: str = None) -> str:\n",
    "    \"\"\"RAG with optional section filtering.\"\"\"\n",
    "    # Get more results to allow filtering\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # Filter by section if specified\n",
    "    if section_filter:\n",
    "        docs = [d for d in docs if d.metadata.get(\"section\") == section_filter]\n",
    "    \n",
    "    if not docs:\n",
    "        return \"No encontrÃ© informaciÃ³n relevante en esa secciÃ³n.\"\n",
    "    \n",
    "    context = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # Use LLM to answer\n",
    "    prompt = f\"Contexto: {context}\\n\\nPregunta: {question}\\nRespuesta:\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# Test\n",
    "# print(rag_with_filter(\"Â¿CuÃ¡l es el horario?\", section_filter=\"contacto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos aprendido:\n",
    "\n",
    "1. **RAG**: Combina retrieval + LLM para respuestas informadas\n",
    "2. **Arquitectura**: IndexaciÃ³n â†’ Retrieval â†’ Generation\n",
    "3. **ImplementaciÃ³n**: Paso a paso y con LangChain\n",
    "4. **Memoria**: RAG conversacional con historial\n",
    "5. **CitaciÃ³n**: Incluir fuentes en respuestas\n",
    "\n",
    "### Flujo tÃ­pico de RAG\n",
    "\n",
    "```\n",
    "1. Usuario hace pregunta\n",
    "2. Pregunta â†’ Embedding â†’ Buscar en Vector Store\n",
    "3. Recuperar documentos relevantes\n",
    "4. Construir prompt con contexto\n",
    "5. LLM genera respuesta basada en contexto\n",
    "6. (Opcional) AÃ±adir fuentes/citaciones\n",
    "```\n",
    "\n",
    "En el siguiente notebook veremos **Reranking** para mejorar la calidad del retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [RAG Paper (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)\n",
    "- [LangChain RAG Guide](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "ipykernel                   7.2.0\n",
      "langchain_classic           1.0.1\n",
      "langchain_community         0.4.1\n",
      "langchain_core              1.2.9\n",
      "langchain_groq              1.1.2\n",
      "langchain_huggingface       NA\n",
      "session_info                v1.0.1\n",
      "-----\n",
      "IPython             9.10.0\n",
      "jupyter_client      8.8.0\n",
      "jupyter_core        5.9.1\n",
      "-----\n",
      "Python 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)]\n",
      "Windows-11-10.0.26200-SP0\n",
      "-----\n",
      "Session information updated at 2026-02-09 17:09\n"
     ]
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show(html = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_LLM",
   "language": "python",
   "name": "ia_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
